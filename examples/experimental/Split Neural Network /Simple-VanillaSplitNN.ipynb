{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.autograd import Variable\n",
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "# import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def location(self):\n",
    "    m = self.__getitem__(0)\n",
    "    w = m.weight[0]\n",
    "    return w.location\n",
    "\n",
    "nn.Sequential.location = location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "x = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "x.requires_grad_()\n",
    "target = torch.tensor([[0],[0],[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[1],[1.]])\n",
    "\n",
    "\n",
    "#   Variables for performance metrics\n",
    "epochs = 20\n",
    "lr = 0.2\n",
    "counter = 0\n",
    "\n",
    "# Define 2 chained models\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(4, 3),\n",
    "        nn.Tanh()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(3, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create optimisers for each segment and link to their segment\n",
    "optimizers = [\n",
    "    optim.SGD(params=model.parameters(),lr=lr)\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "workers = alice, bob\n",
    "\n",
    "# Send Model Segments and Data to starting locations\n",
    "model_locations = [alice, bob]\n",
    "\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)\n",
    "    \n",
    "x = x.send(models[0].location)\n",
    "target = target.send(models[1].location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.5728,  0.3337,  0.6599],\n",
      "        [-0.6832,  0.6359,  0.8446],\n",
      "        [-0.6823,  0.5808,  0.8423],\n",
      "        [-0.6931,  0.3845,  0.8503],\n",
      "        [-0.7686,  0.7887,  0.9320],\n",
      "        [-0.7768,  0.6694,  0.9356],\n",
      "        [-0.7761,  0.6182,  0.9346],\n",
      "        [-0.8394,  0.8098,  0.9726],\n",
      "        [-0.6828,  0.5196,  0.8590],\n",
      "        [-0.7689,  0.7531,  0.9395],\n",
      "        [-0.7682,  0.7127,  0.9385],\n",
      "        [-0.7765,  0.5610,  0.9418],\n",
      "        [-0.8335,  0.8609,  0.9743],\n",
      "        [-0.8396,  0.7773,  0.9757],\n",
      "        [-0.8391,  0.7403,  0.9753],\n",
      "        [-0.8857,  0.8753,  0.9898]], requires_grad=True)\n",
      "tensor([[-0.5728,  0.3337,  0.6599],\n",
      "        [-0.6832,  0.6359,  0.8446],\n",
      "        [-0.6823,  0.5808,  0.8423],\n",
      "        [-0.6931,  0.3845,  0.8503],\n",
      "        [-0.7686,  0.7887,  0.9320],\n",
      "        [-0.7768,  0.6694,  0.9356],\n",
      "        [-0.7761,  0.6182,  0.9346],\n",
      "        [-0.8394,  0.8098,  0.9726],\n",
      "        [-0.6828,  0.5196,  0.8590],\n",
      "        [-0.7689,  0.7531,  0.9395],\n",
      "        [-0.7682,  0.7127,  0.9385],\n",
      "        [-0.7765,  0.5610,  0.9418],\n",
      "        [-0.8335,  0.8609,  0.9743],\n",
      "        [-0.8396,  0.7773,  0.9757],\n",
      "        [-0.8391,  0.7403,  0.9753],\n",
      "        [-0.8857,  0.8753,  0.9898]], requires_grad=True)\n",
      "tensor(4.0116)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[ 0.3164, -0.6033,  0.3347],\n",
      "        [ 0.5318, -0.6477,  0.5650],\n",
      "        [ 0.5333, -0.7028,  0.5587],\n",
      "        [ 0.5149, -0.8203,  0.5809],\n",
      "        [ 0.6962, -0.7379,  0.7273],\n",
      "        [ 0.6829, -0.8428,  0.7424],\n",
      "        [ 0.6840, -0.8698,  0.7382],\n",
      "        [ 0.8011, -0.8865,  0.8451],\n",
      "        [ 0.5325, -0.7493,  0.6052],\n",
      "        [ 0.6956, -0.7796,  0.7588],\n",
      "        [ 0.6967, -0.8164,  0.7548],\n",
      "        [ 0.6834, -0.8918,  0.7687],\n",
      "        [ 0.8096, -0.8393,  0.8555],\n",
      "        [ 0.8007, -0.9058,  0.8640],\n",
      "        [ 0.8014, -0.9225,  0.8617],\n",
      "        [ 0.8781, -0.9326,  0.9204]], requires_grad=True)\n",
      "tensor([[ 0.3164, -0.6033,  0.3347],\n",
      "        [ 0.5318, -0.6477,  0.5650],\n",
      "        [ 0.5333, -0.7028,  0.5587],\n",
      "        [ 0.5149, -0.8203,  0.5809],\n",
      "        [ 0.6962, -0.7379,  0.7273],\n",
      "        [ 0.6829, -0.8428,  0.7424],\n",
      "        [ 0.6840, -0.8698,  0.7382],\n",
      "        [ 0.8011, -0.8865,  0.8451],\n",
      "        [ 0.5325, -0.7493,  0.6052],\n",
      "        [ 0.6956, -0.7796,  0.7588],\n",
      "        [ 0.6967, -0.8164,  0.7548],\n",
      "        [ 0.6834, -0.8918,  0.7687],\n",
      "        [ 0.8096, -0.8393,  0.8555],\n",
      "        [ 0.8007, -0.9058,  0.8640],\n",
      "        [ 0.8014, -0.9225,  0.8617],\n",
      "        [ 0.8781, -0.9326,  0.9204]], requires_grad=True)\n",
      "tensor(3.9711)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.6266,  0.1258, -0.5538],\n",
      "        [-0.7507,  0.4222, -0.6495],\n",
      "        [-0.7496,  0.3154, -0.6557],\n",
      "        [-0.7619, -0.0293, -0.6330],\n",
      "        [-0.8369,  0.5719, -0.7333],\n",
      "        [-0.8453,  0.2863, -0.7148],\n",
      "        [-0.8446,  0.1691, -0.7202],\n",
      "        [-0.9006,  0.4578, -0.7851],\n",
      "        [-0.7502,  0.2023, -0.6055],\n",
      "        [-0.8372,  0.4846, -0.6923],\n",
      "        [-0.8365,  0.3843, -0.6979],\n",
      "        [-0.8449,  0.0492, -0.6774],\n",
      "        [-0.8953,  0.6224, -0.7673],\n",
      "        [-0.9009,  0.3567, -0.7509],\n",
      "        [-0.9004,  0.2443, -0.7556],\n",
      "        [-0.9370,  0.5177, -0.8132]], requires_grad=True)\n",
      "tensor([[-0.6266,  0.1258, -0.5538],\n",
      "        [-0.7507,  0.4222, -0.6495],\n",
      "        [-0.7496,  0.3154, -0.6557],\n",
      "        [-0.7619, -0.0293, -0.6330],\n",
      "        [-0.8369,  0.5719, -0.7333],\n",
      "        [-0.8453,  0.2863, -0.7148],\n",
      "        [-0.8446,  0.1691, -0.7202],\n",
      "        [-0.9006,  0.4578, -0.7851],\n",
      "        [-0.7502,  0.2023, -0.6055],\n",
      "        [-0.8372,  0.4846, -0.6923],\n",
      "        [-0.8365,  0.3843, -0.6979],\n",
      "        [-0.8449,  0.0492, -0.6774],\n",
      "        [-0.8953,  0.6224, -0.7673],\n",
      "        [-0.9009,  0.3567, -0.7509],\n",
      "        [-0.9004,  0.2443, -0.7556],\n",
      "        [-0.9370,  0.5177, -0.8132]], requires_grad=True)\n",
      "tensor(4.0559)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[ 0.0569, -0.6056,  0.4528],\n",
      "        [ 0.1574, -0.7342,  0.6996],\n",
      "        [ 0.1603, -0.7634,  0.6929],\n",
      "        [ 0.1253, -0.8347,  0.7161],\n",
      "        [ 0.2576, -0.8455,  0.8431],\n",
      "        [ 0.2239, -0.8936,  0.8559],\n",
      "        [ 0.2267, -0.9062,  0.8524],\n",
      "        [ 0.3208, -0.9404,  0.9279],\n",
      "        [ 0.1588, -0.7902,  0.7409],\n",
      "        [ 0.2561, -0.8637,  0.8695],\n",
      "        [ 0.2589, -0.8797,  0.8662],\n",
      "        [ 0.2253, -0.9176,  0.8773],\n",
      "        [ 0.3512, -0.9232,  0.9349],\n",
      "        [ 0.3194, -0.9478,  0.9405],\n",
      "        [ 0.3221, -0.9541,  0.9390],\n",
      "        [ 0.4102, -0.9711,  0.9709]], requires_grad=True)\n",
      "tensor([[ 0.0569, -0.6056,  0.4528],\n",
      "        [ 0.1574, -0.7342,  0.6996],\n",
      "        [ 0.1603, -0.7634,  0.6929],\n",
      "        [ 0.1253, -0.8347,  0.7161],\n",
      "        [ 0.2576, -0.8455,  0.8431],\n",
      "        [ 0.2239, -0.8936,  0.8559],\n",
      "        [ 0.2267, -0.9062,  0.8524],\n",
      "        [ 0.3208, -0.9404,  0.9279],\n",
      "        [ 0.1588, -0.7902,  0.7409],\n",
      "        [ 0.2561, -0.8637,  0.8695],\n",
      "        [ 0.2589, -0.8797,  0.8662],\n",
      "        [ 0.2253, -0.9176,  0.8773],\n",
      "        [ 0.3512, -0.9232,  0.9349],\n",
      "        [ 0.3194, -0.9478,  0.9405],\n",
      "        [ 0.3221, -0.9541,  0.9390],\n",
      "        [ 0.4102, -0.9711,  0.9709]], requires_grad=True)\n",
      "tensor(3.9972)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.5697, -0.0580, -0.2312],\n",
      "        [-0.7439, -0.0188, -0.1467],\n",
      "        [-0.7434, -0.0989, -0.1621],\n",
      "        [-0.7495, -0.3296, -0.1071],\n",
      "        [-0.8538, -0.0600, -0.0757],\n",
      "        [-0.8576, -0.2942, -0.0197],\n",
      "        [-0.8573, -0.3658, -0.0355],\n",
      "        [-0.9209, -0.3314,  0.0521],\n",
      "        [-0.7437, -0.1793, -0.0427],\n",
      "        [-0.8540, -0.1411,  0.0450],\n",
      "        [-0.8537, -0.2188,  0.0292],\n",
      "        [-0.8574, -0.4346,  0.0851],\n",
      "        [-0.9189, -0.1812,  0.1164],\n",
      "        [-0.9210, -0.4023,  0.1714],\n",
      "        [-0.9208, -0.4674,  0.1560],\n",
      "        [-0.9568, -0.4362,  0.2402]], requires_grad=True)\n",
      "tensor([[-0.5697, -0.0580, -0.2312],\n",
      "        [-0.7439, -0.0188, -0.1467],\n",
      "        [-0.7434, -0.0989, -0.1621],\n",
      "        [-0.7495, -0.3296, -0.1071],\n",
      "        [-0.8538, -0.0600, -0.0757],\n",
      "        [-0.8576, -0.2942, -0.0197],\n",
      "        [-0.8573, -0.3658, -0.0355],\n",
      "        [-0.9209, -0.3314,  0.0521],\n",
      "        [-0.7437, -0.1793, -0.0427],\n",
      "        [-0.8540, -0.1411,  0.0450],\n",
      "        [-0.8537, -0.2188,  0.0292],\n",
      "        [-0.8574, -0.4346,  0.0851],\n",
      "        [-0.9189, -0.1812,  0.1164],\n",
      "        [-0.9210, -0.4023,  0.1714],\n",
      "        [-0.9208, -0.4674,  0.1560],\n",
      "        [-0.9568, -0.4362,  0.2402]], requires_grad=True)\n",
      "tensor(3.9611)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[ 0.0985,  0.5679, -0.2447],\n",
      "        [ 0.0915,  0.7669, -0.2309],\n",
      "        [ 0.0929,  0.7520, -0.2346],\n",
      "        [ 0.0759,  0.7047, -0.2220],\n",
      "        [ 0.0859,  0.8731, -0.2208],\n",
      "        [ 0.0688,  0.8469, -0.2081],\n",
      "        [ 0.0703,  0.8366, -0.2118],\n",
      "        [ 0.0632,  0.9183, -0.1979],\n",
      "        [ 0.0922,  0.7361, -0.2090],\n",
      "        [ 0.0851,  0.8644, -0.1950],\n",
      "        [ 0.0865,  0.8552, -0.1987],\n",
      "        [ 0.0695,  0.8256, -0.1859],\n",
      "        [ 0.0795,  0.9280, -0.1847],\n",
      "        [ 0.0625,  0.9126, -0.1719],\n",
      "        [ 0.0639,  0.9065, -0.1756],\n",
      "        [ 0.0568,  0.9541, -0.1614]], requires_grad=True)\n",
      "tensor([[ 0.0985,  0.5679, -0.2447],\n",
      "        [ 0.0915,  0.7669, -0.2309],\n",
      "        [ 0.0929,  0.7520, -0.2346],\n",
      "        [ 0.0759,  0.7047, -0.2220],\n",
      "        [ 0.0859,  0.8731, -0.2208],\n",
      "        [ 0.0688,  0.8469, -0.2081],\n",
      "        [ 0.0703,  0.8366, -0.2118],\n",
      "        [ 0.0632,  0.9183, -0.1979],\n",
      "        [ 0.0922,  0.7361, -0.2090],\n",
      "        [ 0.0851,  0.8644, -0.1950],\n",
      "        [ 0.0865,  0.8552, -0.1987],\n",
      "        [ 0.0695,  0.8256, -0.1859],\n",
      "        [ 0.0795,  0.9280, -0.1847],\n",
      "        [ 0.0625,  0.9126, -0.1719],\n",
      "        [ 0.0639,  0.9065, -0.1756],\n",
      "        [ 0.0568,  0.9541, -0.1614]], requires_grad=True)\n",
      "tensor(3.9957)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.1471, -0.1153,  0.3565],\n",
      "        [-0.2666, -0.0458,  0.5973],\n",
      "        [-0.2663, -0.0895,  0.5965],\n",
      "        [-0.2698, -0.2108,  0.5993],\n",
      "        [-0.3782, -0.0197,  0.7632],\n",
      "        [-0.3814, -0.1431,  0.7650],\n",
      "        [-0.3811, -0.1857,  0.7645],\n",
      "        [-0.4826, -0.1174,  0.8675],\n",
      "        [-0.2665, -0.1329,  0.6022],\n",
      "        [-0.3783, -0.0637,  0.7669],\n",
      "        [-0.3780, -0.1072,  0.7664],\n",
      "        [-0.3813, -0.2279,  0.7681],\n",
      "        [-0.4799, -0.0376,  0.8686],\n",
      "        [-0.4828, -0.1606,  0.8697],\n",
      "        [-0.4825, -0.2030,  0.8694],\n",
      "        [-0.5726, -0.1350,  0.9284]], requires_grad=True)\n",
      "tensor([[-0.1471, -0.1153,  0.3565],\n",
      "        [-0.2666, -0.0458,  0.5973],\n",
      "        [-0.2663, -0.0895,  0.5965],\n",
      "        [-0.2698, -0.2108,  0.5993],\n",
      "        [-0.3782, -0.0197,  0.7632],\n",
      "        [-0.3814, -0.1431,  0.7650],\n",
      "        [-0.3811, -0.1857,  0.7645],\n",
      "        [-0.4826, -0.1174,  0.8675],\n",
      "        [-0.2665, -0.1329,  0.6022],\n",
      "        [-0.3783, -0.0637,  0.7669],\n",
      "        [-0.3780, -0.1072,  0.7664],\n",
      "        [-0.3813, -0.2279,  0.7681],\n",
      "        [-0.4799, -0.0376,  0.8686],\n",
      "        [-0.4828, -0.1606,  0.8697],\n",
      "        [-0.4825, -0.2030,  0.8694],\n",
      "        [-0.5726, -0.1350,  0.9284]], requires_grad=True)\n",
      "tensor(3.9727)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[ 0.6869,  0.2660, -0.5174],\n",
      "        [ 0.8512,  0.4501, -0.5899],\n",
      "        [ 0.8512,  0.4410, -0.5908],\n",
      "        [ 0.8506,  0.4150, -0.5874],\n",
      "        [ 0.9327,  0.5953, -0.6548],\n",
      "        [ 0.9324,  0.5743, -0.6518],\n",
      "        [ 0.9324,  0.5667, -0.6527],\n",
      "        [ 0.9702,  0.6936, -0.7087],\n",
      "        [ 0.8512,  0.4318, -0.5839],\n",
      "        [ 0.9327,  0.5878, -0.6487],\n",
      "        [ 0.9327,  0.5804, -0.6495],\n",
      "        [ 0.9324,  0.5588, -0.6465],\n",
      "        [ 0.9703,  0.7041, -0.7060],\n",
      "        [ 0.9702,  0.6876, -0.7034],\n",
      "        [ 0.9702,  0.6816, -0.7041],\n",
      "        [ 0.9870,  0.7796, -0.7531]], requires_grad=True)\n",
      "tensor([[ 0.6869,  0.2660, -0.5174],\n",
      "        [ 0.8512,  0.4501, -0.5899],\n",
      "        [ 0.8512,  0.4410, -0.5908],\n",
      "        [ 0.8506,  0.4150, -0.5874],\n",
      "        [ 0.9327,  0.5953, -0.6548],\n",
      "        [ 0.9324,  0.5743, -0.6518],\n",
      "        [ 0.9324,  0.5667, -0.6527],\n",
      "        [ 0.9702,  0.6936, -0.7087],\n",
      "        [ 0.8512,  0.4318, -0.5839],\n",
      "        [ 0.9327,  0.5878, -0.6487],\n",
      "        [ 0.9327,  0.5804, -0.6495],\n",
      "        [ 0.9324,  0.5588, -0.6465],\n",
      "        [ 0.9703,  0.7041, -0.7060],\n",
      "        [ 0.9702,  0.6876, -0.7034],\n",
      "        [ 0.9702,  0.6816, -0.7041],\n",
      "        [ 0.9870,  0.7796, -0.7531]], requires_grad=True)\n",
      "tensor(4.0615)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[ 0.3673, -0.7013,  0.5439],\n",
      "        [ 0.5679, -0.8446,  0.7948],\n",
      "        [ 0.5681, -0.8477,  0.7942],\n",
      "        [ 0.5663, -0.8562,  0.7964],\n",
      "        [ 0.7182, -0.9239,  0.9150],\n",
      "        [ 0.7169, -0.9283,  0.9160],\n",
      "        [ 0.7170, -0.9298,  0.9157],\n",
      "        [ 0.8212, -0.9657,  0.9665],\n",
      "        [ 0.5680, -0.8508,  0.7986],\n",
      "        [ 0.7181, -0.9255,  0.9169],\n",
      "        [ 0.7182, -0.9271,  0.9167],\n",
      "        [ 0.7169, -0.9313,  0.9176],\n",
      "        [ 0.8221, -0.9643,  0.9669],\n",
      "        [ 0.8212, -0.9664,  0.9673],\n",
      "        [ 0.8213, -0.9671,  0.9672],\n",
      "        [ 0.8896, -0.9841,  0.9872]], requires_grad=True)\n",
      "tensor([[ 0.3673, -0.7013,  0.5439],\n",
      "        [ 0.5679, -0.8446,  0.7948],\n",
      "        [ 0.5681, -0.8477,  0.7942],\n",
      "        [ 0.5663, -0.8562,  0.7964],\n",
      "        [ 0.7182, -0.9239,  0.9150],\n",
      "        [ 0.7169, -0.9283,  0.9160],\n",
      "        [ 0.7170, -0.9298,  0.9157],\n",
      "        [ 0.8212, -0.9657,  0.9665],\n",
      "        [ 0.5680, -0.8508,  0.7986],\n",
      "        [ 0.7181, -0.9255,  0.9169],\n",
      "        [ 0.7182, -0.9271,  0.9167],\n",
      "        [ 0.7169, -0.9313,  0.9176],\n",
      "        [ 0.8221, -0.9643,  0.9669],\n",
      "        [ 0.8212, -0.9664,  0.9673],\n",
      "        [ 0.8213, -0.9671,  0.9672],\n",
      "        [ 0.8896, -0.9841,  0.9872]], requires_grad=True)\n",
      "tensor(3.9840)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.5748, -0.3769,  0.0680],\n",
      "        [-0.7057, -0.5300,  0.3391],\n",
      "        [-0.7056, -0.5393,  0.3374],\n",
      "        [-0.7072, -0.5647,  0.3435],\n",
      "        [-0.8013, -0.6623,  0.5623],\n",
      "        [-0.8024, -0.6823,  0.5670],\n",
      "        [-0.8023, -0.6892,  0.5656],\n",
      "        [-0.8690, -0.7779,  0.7288],\n",
      "        [-0.7056, -0.5486,  0.3498],\n",
      "        [-0.8014, -0.6696,  0.5718],\n",
      "        [-0.8013, -0.6767,  0.5705],\n",
      "        [-0.8024, -0.6960,  0.5751],\n",
      "        [-0.8683, -0.7685,  0.7321],\n",
      "        [-0.8691, -0.7831,  0.7353],\n",
      "        [-0.8690, -0.7880,  0.7344],\n",
      "        [-0.9143, -0.8511,  0.8406]], requires_grad=True)\n",
      "tensor([[-0.5748, -0.3769,  0.0680],\n",
      "        [-0.7057, -0.5300,  0.3391],\n",
      "        [-0.7056, -0.5393,  0.3374],\n",
      "        [-0.7072, -0.5647,  0.3435],\n",
      "        [-0.8013, -0.6623,  0.5623],\n",
      "        [-0.8024, -0.6823,  0.5670],\n",
      "        [-0.8023, -0.6892,  0.5656],\n",
      "        [-0.8690, -0.7779,  0.7288],\n",
      "        [-0.7056, -0.5486,  0.3498],\n",
      "        [-0.8014, -0.6696,  0.5718],\n",
      "        [-0.8013, -0.6767,  0.5705],\n",
      "        [-0.8024, -0.6960,  0.5751],\n",
      "        [-0.8683, -0.7685,  0.7321],\n",
      "        [-0.8691, -0.7831,  0.7353],\n",
      "        [-0.8690, -0.7880,  0.7344],\n",
      "        [-0.9143, -0.8511,  0.8406]], requires_grad=True)\n",
      "tensor(3.9904)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[ 0.2445,  0.6074, -0.7486],\n",
      "        [ 0.4021,  0.7780, -0.8422],\n",
      "        [ 0.4023,  0.7722, -0.8428],\n",
      "        [ 0.3990,  0.7549, -0.8409],\n",
      "        [ 0.5392,  0.8767, -0.9033],\n",
      "        [ 0.5364,  0.8667, -0.9021],\n",
      "        [ 0.5366,  0.8630, -0.9024],\n",
      "        [ 0.6504,  0.9276, -0.9407],\n",
      "        [ 0.4022,  0.7661, -0.8389],\n",
      "        [ 0.5391,  0.8732, -0.9008],\n",
      "        [ 0.5393,  0.8697, -0.9012],\n",
      "        [ 0.5365,  0.8592, -0.8999],\n",
      "        [ 0.6525,  0.9312, -0.9399],\n",
      "        [ 0.6503,  0.9254, -0.9392],\n",
      "        [ 0.6505,  0.9233, -0.9394],\n",
      "        [ 0.7410,  0.9601, -0.9634]], requires_grad=True)\n",
      "tensor([[ 0.2445,  0.6074, -0.7486],\n",
      "        [ 0.4021,  0.7780, -0.8422],\n",
      "        [ 0.4023,  0.7722, -0.8428],\n",
      "        [ 0.3990,  0.7549, -0.8409],\n",
      "        [ 0.5392,  0.8767, -0.9033],\n",
      "        [ 0.5364,  0.8667, -0.9021],\n",
      "        [ 0.5366,  0.8630, -0.9024],\n",
      "        [ 0.6504,  0.9276, -0.9407],\n",
      "        [ 0.4022,  0.7661, -0.8389],\n",
      "        [ 0.5391,  0.8732, -0.9008],\n",
      "        [ 0.5393,  0.8697, -0.9012],\n",
      "        [ 0.5365,  0.8592, -0.8999],\n",
      "        [ 0.6525,  0.9312, -0.9399],\n",
      "        [ 0.6503,  0.9254, -0.9392],\n",
      "        [ 0.6505,  0.9233, -0.9394],\n",
      "        [ 0.7410,  0.9601, -0.9634]], requires_grad=True)\n",
      "tensor(4.0180)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-7.1211e-01, -2.6226e-06, -3.8259e-01],\n",
      "        [-8.6264e-01,  5.6583e-02, -4.0758e-01],\n",
      "        [-8.6256e-01,  3.8574e-02, -4.0946e-01],\n",
      "        [-8.6347e-01, -1.2454e-02, -4.0293e-01],\n",
      "        [-9.3730e-01,  9.4952e-02, -4.3381e-01],\n",
      "        [-9.3773e-01,  4.4163e-02, -4.2744e-01],\n",
      "        [-9.3770e-01,  2.6135e-02, -4.2928e-01],\n",
      "        [-9.7219e-01,  8.2598e-02, -4.5313e-01],\n",
      "        [-8.6260e-01,  2.0274e-02, -3.9613e-01],\n",
      "        [-9.3732e-01,  7.6771e-02, -4.2080e-01],\n",
      "        [-9.3729e-01,  5.8804e-02, -4.2265e-01],\n",
      "        [-9.3772e-01,  7.8242e-03, -4.1621e-01],\n",
      "        [-9.7200e-01,  1.1501e-01, -4.4667e-01],\n",
      "        [-9.7220e-01,  6.4381e-02, -4.4040e-01],\n",
      "        [-9.7218e-01,  4.6386e-02, -4.4221e-01],\n",
      "        [-9.8770e-01,  1.0270e-01, -4.6572e-01]], requires_grad=True)\n",
      "tensor([[-7.1211e-01, -2.6226e-06, -3.8259e-01],\n",
      "        [-8.6264e-01,  5.6583e-02, -4.0758e-01],\n",
      "        [-8.6256e-01,  3.8574e-02, -4.0946e-01],\n",
      "        [-8.6347e-01, -1.2454e-02, -4.0293e-01],\n",
      "        [-9.3730e-01,  9.4952e-02, -4.3381e-01],\n",
      "        [-9.3773e-01,  4.4163e-02, -4.2744e-01],\n",
      "        [-9.3770e-01,  2.6135e-02, -4.2928e-01],\n",
      "        [-9.7219e-01,  8.2598e-02, -4.5313e-01],\n",
      "        [-8.6260e-01,  2.0274e-02, -3.9613e-01],\n",
      "        [-9.3732e-01,  7.6771e-02, -4.2080e-01],\n",
      "        [-9.3729e-01,  5.8804e-02, -4.2265e-01],\n",
      "        [-9.3772e-01,  7.8242e-03, -4.1621e-01],\n",
      "        [-9.7200e-01,  1.1501e-01, -4.4667e-01],\n",
      "        [-9.7220e-01,  6.4381e-02, -4.4040e-01],\n",
      "        [-9.7218e-01,  4.6386e-02, -4.4221e-01],\n",
      "        [-9.8770e-01,  1.0270e-01, -4.6572e-01]], requires_grad=True)\n",
      "tensor(4.0188)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.4310, -0.1620,  0.6101],\n",
      "        [-0.6194, -0.2291,  0.8467],\n",
      "        [-0.6192, -0.2327,  0.8463],\n",
      "        [-0.6217, -0.2429,  0.8478],\n",
      "        [-0.7559, -0.2975,  0.9445],\n",
      "        [-0.7577, -0.3074,  0.9451],\n",
      "        [-0.7575, -0.3108,  0.9449],\n",
      "        [-0.8492, -0.3724,  0.9808],\n",
      "        [-0.6193, -0.2363,  0.8494],\n",
      "        [-0.7560, -0.3010,  0.9457],\n",
      "        [-0.7558, -0.3045,  0.9455],\n",
      "        [-0.7576, -0.3143,  0.9461],\n",
      "        [-0.8481, -0.3664,  0.9810],\n",
      "        [-0.8493, -0.3757,  0.9812],\n",
      "        [-0.8492, -0.3789,  0.9811],\n",
      "        [-0.9080, -0.4371,  0.9935]], requires_grad=True)\n",
      "tensor([[-0.4310, -0.1620,  0.6101],\n",
      "        [-0.6194, -0.2291,  0.8467],\n",
      "        [-0.6192, -0.2327,  0.8463],\n",
      "        [-0.6217, -0.2429,  0.8478],\n",
      "        [-0.7559, -0.2975,  0.9445],\n",
      "        [-0.7577, -0.3074,  0.9451],\n",
      "        [-0.7575, -0.3108,  0.9449],\n",
      "        [-0.8492, -0.3724,  0.9808],\n",
      "        [-0.6193, -0.2363,  0.8494],\n",
      "        [-0.7560, -0.3010,  0.9457],\n",
      "        [-0.7558, -0.3045,  0.9455],\n",
      "        [-0.7576, -0.3143,  0.9461],\n",
      "        [-0.8481, -0.3664,  0.9810],\n",
      "        [-0.8493, -0.3757,  0.9812],\n",
      "        [-0.8492, -0.3789,  0.9811],\n",
      "        [-0.9080, -0.4371,  0.9935]], requires_grad=True)\n",
      "tensor(4.0343)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[0.4808, 0.6084, 0.2810],\n",
      "        [0.6081, 0.8032, 0.5986],\n",
      "        [0.6083, 0.8025, 0.5974],\n",
      "        [0.6052, 0.8006, 0.6015],\n",
      "        [0.7104, 0.9064, 0.7974],\n",
      "        [0.7079, 0.9054, 0.7996],\n",
      "        [0.7081, 0.9051, 0.7990],\n",
      "        [0.7876, 0.9563, 0.9048],\n",
      "        [0.6082, 0.8018, 0.6056],\n",
      "        [0.7103, 0.9060, 0.8020],\n",
      "        [0.7105, 0.9057, 0.8013],\n",
      "        [0.7080, 0.9048, 0.8036],\n",
      "        [0.7894, 0.9566, 0.9060],\n",
      "        [0.7876, 0.9561, 0.9071],\n",
      "        [0.7877, 0.9560, 0.9068],\n",
      "        [0.8475, 0.9800, 0.9572]], requires_grad=True)\n",
      "tensor([[0.4808, 0.6084, 0.2810],\n",
      "        [0.6081, 0.8032, 0.5986],\n",
      "        [0.6083, 0.8025, 0.5974],\n",
      "        [0.6052, 0.8006, 0.6015],\n",
      "        [0.7104, 0.9064, 0.7974],\n",
      "        [0.7079, 0.9054, 0.7996],\n",
      "        [0.7081, 0.9051, 0.7990],\n",
      "        [0.7876, 0.9563, 0.9048],\n",
      "        [0.6082, 0.8018, 0.6056],\n",
      "        [0.7103, 0.9060, 0.8020],\n",
      "        [0.7105, 0.9057, 0.8013],\n",
      "        [0.7080, 0.9048, 0.8036],\n",
      "        [0.7894, 0.9566, 0.9060],\n",
      "        [0.7876, 0.9561, 0.9071],\n",
      "        [0.7877, 0.9560, 0.9068],\n",
      "        [0.8475, 0.9800, 0.9572]], requires_grad=True)\n",
      "tensor(4.0094)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.5125,  0.1312, -0.5086],\n",
      "        [-0.7160,  0.3112, -0.4792],\n",
      "        [-0.7157,  0.3093, -0.4809],\n",
      "        [-0.7186,  0.3036, -0.4751],\n",
      "        [-0.8431,  0.4697, -0.4505],\n",
      "        [-0.8448,  0.4648, -0.4445],\n",
      "        [-0.8447,  0.4631, -0.4462],\n",
      "        [-0.9171,  0.5987, -0.4145],\n",
      "        [-0.7158,  0.3072, -0.4690],\n",
      "        [-0.8432,  0.4679, -0.4382],\n",
      "        [-0.8431,  0.4662, -0.4399],\n",
      "        [-0.8448,  0.4613, -0.4338],\n",
      "        [-0.9162,  0.6013, -0.4080],\n",
      "        [-0.9171,  0.5972, -0.4018],\n",
      "        [-0.9171,  0.5958, -0.4036],\n",
      "        [-0.9565,  0.7047, -0.3705]], requires_grad=True)\n",
      "tensor([[-0.5125,  0.1312, -0.5086],\n",
      "        [-0.7160,  0.3112, -0.4792],\n",
      "        [-0.7157,  0.3093, -0.4809],\n",
      "        [-0.7186,  0.3036, -0.4751],\n",
      "        [-0.8431,  0.4697, -0.4505],\n",
      "        [-0.8448,  0.4648, -0.4445],\n",
      "        [-0.8447,  0.4631, -0.4462],\n",
      "        [-0.9171,  0.5987, -0.4145],\n",
      "        [-0.7158,  0.3072, -0.4690],\n",
      "        [-0.8432,  0.4679, -0.4382],\n",
      "        [-0.8431,  0.4662, -0.4399],\n",
      "        [-0.8448,  0.4613, -0.4338],\n",
      "        [-0.9162,  0.6013, -0.4080],\n",
      "        [-0.9171,  0.5972, -0.4018],\n",
      "        [-0.9171,  0.5958, -0.4036],\n",
      "        [-0.9565,  0.7047, -0.3705]], requires_grad=True)\n",
      "tensor(3.9838)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[ 0.2097, -0.7318,  0.5164],\n",
      "        [ 0.1968, -0.8666,  0.8230],\n",
      "        [ 0.1974, -0.8670,  0.8225],\n",
      "        [ 0.1904, -0.8682,  0.8243],\n",
      "        [ 0.1843, -0.9364,  0.9424],\n",
      "        [ 0.1773, -0.9370,  0.9430],\n",
      "        [ 0.1779, -0.9372,  0.9428],\n",
      "        [ 0.1648, -0.9705,  0.9822],\n",
      "        [ 0.1970, -0.8675,  0.8261],\n",
      "        [ 0.1840, -0.9366,  0.9437],\n",
      "        [ 0.1846, -0.9368,  0.9435],\n",
      "        [ 0.1776, -0.9374,  0.9441],\n",
      "        [ 0.1715, -0.9703,  0.9824],\n",
      "        [ 0.1645, -0.9706,  0.9826],\n",
      "        [ 0.1651, -0.9707,  0.9826],\n",
      "        [ 0.1519, -0.9864,  0.9947]], requires_grad=True)\n",
      "tensor([[ 0.2097, -0.7318,  0.5164],\n",
      "        [ 0.1968, -0.8666,  0.8230],\n",
      "        [ 0.1974, -0.8670,  0.8225],\n",
      "        [ 0.1904, -0.8682,  0.8243],\n",
      "        [ 0.1843, -0.9364,  0.9424],\n",
      "        [ 0.1773, -0.9370,  0.9430],\n",
      "        [ 0.1779, -0.9372,  0.9428],\n",
      "        [ 0.1648, -0.9705,  0.9822],\n",
      "        [ 0.1970, -0.8675,  0.8261],\n",
      "        [ 0.1840, -0.9366,  0.9437],\n",
      "        [ 0.1846, -0.9368,  0.9435],\n",
      "        [ 0.1776, -0.9374,  0.9441],\n",
      "        [ 0.1715, -0.9703,  0.9824],\n",
      "        [ 0.1645, -0.9706,  0.9826],\n",
      "        [ 0.1651, -0.9707,  0.9826],\n",
      "        [ 0.1519, -0.9864,  0.9947]], requires_grad=True)\n",
      "tensor(4.0001)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.3335, -0.4659,  0.1291],\n",
      "        [-0.5585, -0.6294,  0.5271],\n",
      "        [-0.5584, -0.6306,  0.5258],\n",
      "        [-0.5599, -0.6340,  0.5303],\n",
      "        [-0.7232, -0.7522,  0.7781],\n",
      "        [-0.7243, -0.7546,  0.7806],\n",
      "        [-0.7242, -0.7555,  0.7799],\n",
      "        [-0.8337, -0.8400,  0.9054],\n",
      "        [-0.5584, -0.6318,  0.5351],\n",
      "        [-0.7233, -0.7531,  0.7832],\n",
      "        [-0.7232, -0.7539,  0.7825],\n",
      "        [-0.7242, -0.7563,  0.7849],\n",
      "        [-0.8331, -0.8389,  0.9066],\n",
      "        [-0.8338, -0.8406,  0.9077],\n",
      "        [-0.8337, -0.8412,  0.9074],\n",
      "        [-0.9022, -0.8978,  0.9618]], requires_grad=True)\n",
      "tensor([[-0.3335, -0.4659,  0.1291],\n",
      "        [-0.5585, -0.6294,  0.5271],\n",
      "        [-0.5584, -0.6306,  0.5258],\n",
      "        [-0.5599, -0.6340,  0.5303],\n",
      "        [-0.7232, -0.7522,  0.7781],\n",
      "        [-0.7243, -0.7546,  0.7806],\n",
      "        [-0.7242, -0.7555,  0.7799],\n",
      "        [-0.8337, -0.8400,  0.9054],\n",
      "        [-0.5584, -0.6318,  0.5351],\n",
      "        [-0.7233, -0.7531,  0.7832],\n",
      "        [-0.7232, -0.7539,  0.7825],\n",
      "        [-0.7242, -0.7563,  0.7849],\n",
      "        [-0.8331, -0.8389,  0.9066],\n",
      "        [-0.8338, -0.8406,  0.9077],\n",
      "        [-0.8337, -0.8412,  0.9074],\n",
      "        [-0.9022, -0.8978,  0.9618]], requires_grad=True)\n",
      "tensor(3.9440)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[ 0.5854,  0.4578, -0.6099],\n",
      "        [ 0.6945,  0.6130, -0.5532],\n",
      "        [ 0.6946,  0.6115, -0.5546],\n",
      "        [ 0.6933,  0.6072, -0.5495],\n",
      "        [ 0.7790,  0.7308, -0.4924],\n",
      "        [ 0.7780,  0.7277, -0.4868],\n",
      "        [ 0.7781,  0.7265, -0.4884],\n",
      "        [ 0.8416,  0.8146, -0.4203],\n",
      "        [ 0.6946,  0.6100, -0.5441],\n",
      "        [ 0.7790,  0.7297, -0.4809],\n",
      "        [ 0.7791,  0.7286, -0.4825],\n",
      "        [ 0.7781,  0.7254, -0.4769],\n",
      "        [ 0.8423,  0.8161, -0.4140],\n",
      "        [ 0.8415,  0.8138, -0.4079],\n",
      "        [ 0.8416,  0.8130, -0.4096],\n",
      "        [ 0.8881,  0.8752, -0.3358]], requires_grad=True)\n",
      "tensor([[ 0.5854,  0.4578, -0.6099],\n",
      "        [ 0.6945,  0.6130, -0.5532],\n",
      "        [ 0.6946,  0.6115, -0.5546],\n",
      "        [ 0.6933,  0.6072, -0.5495],\n",
      "        [ 0.7790,  0.7308, -0.4924],\n",
      "        [ 0.7780,  0.7277, -0.4868],\n",
      "        [ 0.7781,  0.7265, -0.4884],\n",
      "        [ 0.8416,  0.8146, -0.4203],\n",
      "        [ 0.6946,  0.6100, -0.5441],\n",
      "        [ 0.7790,  0.7297, -0.4809],\n",
      "        [ 0.7791,  0.7286, -0.4825],\n",
      "        [ 0.7781,  0.7254, -0.4769],\n",
      "        [ 0.8423,  0.8161, -0.4140],\n",
      "        [ 0.8415,  0.8138, -0.4079],\n",
      "        [ 0.8416,  0.8130, -0.4096],\n",
      "        [ 0.8881,  0.8752, -0.3358]], requires_grad=True)\n",
      "tensor(3.9922)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[-0.2874, -0.5044,  0.4245],\n",
      "        [-0.5007, -0.6764,  0.8020],\n",
      "        [-0.5005, -0.6780,  0.8013],\n",
      "        [-0.5029, -0.6823,  0.8035],\n",
      "        [-0.6666, -0.7979,  0.9417],\n",
      "        [-0.6683, -0.8008,  0.9424],\n",
      "        [-0.6682, -0.8018,  0.9422],\n",
      "        [-0.7864, -0.8789,  0.9839],\n",
      "        [-0.5006, -0.6795,  0.8057],\n",
      "        [-0.6666, -0.7989,  0.9431],\n",
      "        [-0.6665, -0.8000,  0.9429],\n",
      "        [-0.6683, -0.8028,  0.9436],\n",
      "        [-0.7853, -0.8777,  0.9841],\n",
      "        [-0.7865, -0.8796,  0.9843],\n",
      "        [-0.7864, -0.8802,  0.9843],\n",
      "        [-0.8659, -0.9281,  0.9957]], requires_grad=True)\n",
      "tensor([[-0.2874, -0.5044,  0.4245],\n",
      "        [-0.5007, -0.6764,  0.8020],\n",
      "        [-0.5005, -0.6780,  0.8013],\n",
      "        [-0.5029, -0.6823,  0.8035],\n",
      "        [-0.6666, -0.7979,  0.9417],\n",
      "        [-0.6683, -0.8008,  0.9424],\n",
      "        [-0.6682, -0.8018,  0.9422],\n",
      "        [-0.7864, -0.8789,  0.9839],\n",
      "        [-0.5006, -0.6795,  0.8057],\n",
      "        [-0.6666, -0.7989,  0.9431],\n",
      "        [-0.6665, -0.8000,  0.9429],\n",
      "        [-0.6683, -0.8028,  0.9436],\n",
      "        [-0.7853, -0.8777,  0.9841],\n",
      "        [-0.7865, -0.8796,  0.9843],\n",
      "        [-0.7864, -0.8802,  0.9843],\n",
      "        [-0.8659, -0.9281,  0.9957]], requires_grad=True)\n",
      "tensor(3.9890)\n",
      "\n",
      "\n",
      "Gradient of 'a' after backward() appears to just be a copy of 'a'\n",
      "tensor([[0.6555, 0.3306, 0.0031],\n",
      "        [0.7822, 0.4370, 0.4725],\n",
      "        [0.7824, 0.4342, 0.4709],\n",
      "        [0.7810, 0.4261, 0.4763],\n",
      "        [0.8662, 0.5300, 0.7704],\n",
      "        [0.8653, 0.5228, 0.7732],\n",
      "        [0.8654, 0.5202, 0.7724],\n",
      "        [0.9187, 0.6055, 0.9115],\n",
      "        [0.7823, 0.4313, 0.4818],\n",
      "        [0.8662, 0.5274, 0.7761],\n",
      "        [0.8662, 0.5249, 0.7753],\n",
      "        [0.8654, 0.5176, 0.7780],\n",
      "        [0.9192, 0.6095, 0.9127],\n",
      "        [0.9187, 0.6032, 0.9139],\n",
      "        [0.9187, 0.6010, 0.9135],\n",
      "        [0.9515, 0.6749, 0.9679]], requires_grad=True)\n",
      "tensor([[0.6555, 0.3306, 0.0031],\n",
      "        [0.7822, 0.4370, 0.4725],\n",
      "        [0.7824, 0.4342, 0.4709],\n",
      "        [0.7810, 0.4261, 0.4763],\n",
      "        [0.8662, 0.5300, 0.7704],\n",
      "        [0.8653, 0.5228, 0.7732],\n",
      "        [0.8654, 0.5202, 0.7724],\n",
      "        [0.9187, 0.6055, 0.9115],\n",
      "        [0.7823, 0.4313, 0.4818],\n",
      "        [0.8662, 0.5274, 0.7761],\n",
      "        [0.8662, 0.5249, 0.7753],\n",
      "        [0.8654, 0.5176, 0.7780],\n",
      "        [0.9192, 0.6095, 0.9127],\n",
      "        [0.9187, 0.6032, 0.9139],\n",
      "        [0.9187, 0.6010, 0.9135],\n",
      "        [0.9515, 0.6749, 0.9679]], requires_grad=True)\n",
      "tensor(3.9134)\n"
     ]
    }
   ],
   "source": [
    "# %%pixie_debugger\n",
    "\n",
    "def train():\n",
    "    # Training Logic\n",
    "    for iter in range(epochs):\n",
    "\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        for opt in optimizers:\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        a  = models[0](x)\n",
    "        \n",
    "        # 3) send the activation signal to the next model\n",
    "        a_to_send = a.detach()\n",
    "        remote_a = a_to_send.move(models[1].location)\n",
    "        # re-enable autograd here\n",
    "        remote_a.requires_grad_()\n",
    "\n",
    "        pred =  models[1](remote_a)\n",
    "\n",
    "        # 3) calculate how much we missed\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "#         print(remote_a.location._objects[remote_a.grad.id_at_location])  <-- throws error NoneType hs no location (Expected as no gradients here yet)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # Now the gradient produced by backward seems to be a copy of the original tensor :S\n",
    "        if remote_a.grad.location._objects[remote_a.grad.id_at_location].sum() == remote_a.location._objects[remote_a.id_at_location].sum():\n",
    "            print(\"\\n\\nGradient of 'a' after backward() appears to just be a copy of 'a'\")\n",
    "            print(remote_a.grad.location._objects[remote_a.grad.id_at_location])\n",
    "            print(remote_a.location._objects[remote_a.id_at_location])\n",
    "        \n",
    "        # 5) Backprop gradient to model behind\n",
    "        grad_a = remote_a.grad\n",
    "        grad_a.move(models[0].location)\n",
    "        a.backward(grad_a)\n",
    "\n",
    "\n",
    "        # 5) change the weights\n",
    "        for opt in optimizers:\n",
    "            opt.step()\n",
    "\n",
    "        # 6) print our progress\n",
    "        # Do not use .data\n",
    "        print(loss.detach().get())\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
