{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Neural Network (SplitNN)\n",
    "\n",
    "Traditionally, PySyft has been used to facilitate federated learning. However, we can also leverage the tools included in this framework to implement distributed neural networks. \n",
    "\n",
    "### What is a SplitNN?\n",
    "\n",
    "<img src=\"images/anatomy.png\" width=\"50%\">\n",
    "\n",
    "The training of a neural network (NN) is 'split' accross a chain of multiple hosts. Each segment in the chain is a self contained NN that feeds into the segment in front. The host with the training data has the beginning segment of the network and the end segment. Intermediate segments of the chain are held by participating hosts.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The SplitNN network is assembled as a chain of NNs, each feeding into the next. The data subject has both the beginning and the end of this chain.\n",
    "\n",
    "<img src=\"images/training.png\" width=\"80%\">\n",
    "\n",
    "When forward propogation commences, the data subject propogates the x values forward through the network at the start of the chain and sends their activation signals to the next intermediate host. This host feeds the recieved activation signal forward through their network and to the next link in the chain. This continues until the end of the chain is reached. The data subject then recieves an activation signal and forward propogates. They then compute the loss using their y-values.\n",
    "\n",
    "They backward propogate the gradients of the activation signals they recieved to the host previous to them in the chain. This host then computes their gradients, sending the gradient of the activeation signal backward. Eventually, the data subject recieves the gradients of their activation signal at the start of the chain and computes gradients. \n",
    "\n",
    "The NNs in the chain update their weights and biases, commenceing the next epoch. When a host is finished training, they pass the front and end segments to the next person with data to train.\n",
    "\n",
    "<img src=\"images/BatchFlow.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "### Why use a SplitNN?\n",
    "\n",
    "The SplitNN has been shown to provide a dramatic reduction to the computational burden of training while maintaining higher accuracies when training over large number of clients [[1](https://arxiv.org/abs/1812.00564)]. In the figure below, the Blue line denotes distributed deep learning using splitNN, red line indicate federated learning (FL) and green line indicates Large Batch Stochastic Gradient Descent (LBSGD).\n",
    "\n",
    "<img src=\"images/AccuracyvsFlops.png\" width=\"60%\">\n",
    "\n",
    "<img src=\"images/computation.png\" width=\"40%\">\n",
    " \n",
    "Table 1 shows computational resources consumed when training CIFAR 10 over VGG. Theses are a fraction of the resources of FL and LBSGD. Table 2 shows the bandwith usage when training CIFAR 100 over ResNet. Federated learning is less bandwidth intensive with fewer than 100 clients. However, the SplitNN outperforms other approaches as the number of clients grow[[1](https://arxiv.org/abs/1812.00564)].\n",
    "\n",
    "<img src=\"images/bandwidth.png\" width=\"40%\">\n",
    "\n",
    "Using this technique, nobody knows the input data and labels apart from the data subject. All that is sent or recieved between nodes is activation signals during forward propogation and their corresponding gradients during backpropogation. Entropy can be added to the activation signals through adding layers to the model segments. Entropy of training data could potentially be measured to arrive at the appropriate number of layers to use in order to adequately hide the orignal values in start and end segments.\n",
    "\n",
    "During this process, no hosts involved in the learning process have a full picture of the network. As a result there is very little risk of the model being stolen by participating hosts. Models could only be fully recovered by malicious participants if they were to collude with every other host. \n",
    "\n",
    "### Advantages\n",
    "\n",
    "- The accuracy should be almost identical to a non-split version of the same model, trained locally. \n",
    "- Models and data can be homomorphically encrypted for added security at the cost of added computation.\n",
    "- Model is distributed, meaning all segment holders must consent in order to aggregate the model at the end of training.\n",
    "- The scalability of this approach, in terms of both network and computational resources, could make this an a valid alternative to FL and LBSGD, particularly on low power devices.\n",
    "- Could be an effective mechanism for both horizontal and vertical data distributions.\n",
    "- As computational cost is already quite low, proportianate homomorphic encryption cost is also minimised.\n",
    "- Only activation signal gradients are sent/ recieved, meaning that malicious actors cannot use gradients of model parameters to reverse engineer the original values\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- A new technique with little surroundung literature, a large amount of compartison and evaluation is still to be done.\n",
    "- This approach requires all hosts to remain online during the entire learning process.\n",
    "    - makes approach less fesible for hand-held devices\n",
    "- Less established in toolkits than FL and LBSGD\n",
    "- While most aspects of the learning process are anonymised, the intermediary hosts are need to know the location of those ahead and behind to send and recieve data during learning. Ideally this would provide anonymity to participants.\n",
    "- This approach is less secure with small groups training but becomes more secure as the number of model segments increases\n",
    "\n",
    "### Tutorial \n",
    "\n",
    "The SpliNN has the capacity to be a significant contribution to the growing ecosystem of privacy preserving learning methodologies. This tutorial has two purposes;\n",
    "\n",
    "- To explain in as clear terms as possible what is going on during the training of a SplitNN.\n",
    "- To provide a working example of a SplitNN learning on arbitrary sets of; training data, model segments and data providers.\n",
    "- To provide an implementation of this technique on the PySyft framework so that this can be further validated against other techniques. \n",
    "\n",
    "### Future work\n",
    "\n",
    "- Add homomorphic encryption\n",
    "- A tokenisation infrastructure or masked dns service could be implemented to provide anonymity to hosts during traing. Ideally the difference 'chains' involved here could be written to a smart contract and be publicly available information. \n",
    "\n",
    "Authors:\n",
    "- Adam Hall - Twitter: [@AJH4LL](https://twitter.com/AJH4LL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.1 - A Toy NN Example\n",
    "\n",
    "<img src=\"images/wholeNetwork.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "We will begin by training a normal model to benchmark our SplitNN against. This will be the exact same specification as the SPlitNN onlt not distributed. In order to get as close as possible, we will use the same random seed for intitialisation. The dataset will take in arbitrary dataset of four binary features. The only x instance with a y value that is 1 will be 1111.\n",
    "\n",
    "<img src=\"images/benchmarkExample.png\" width=\"60%\">\n",
    "\n",
    "We will create three identical datasets to simulate the batches for each data owner.\n",
    "\n",
    "<img src=\"images/identicalDatasets.png\" width=\"40%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "from torch.autograd import Variable\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "x = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "y = torch.tensor([[0],[0],[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[1],[1.]])\n",
    "\n",
    "\n",
    "\n",
    "x1 = x\n",
    "x2 = x\n",
    "x3 = x\n",
    "\n",
    "y1 = y\n",
    "y2 = y\n",
    "y3 = y\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# One Model\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(4, 3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(3, 3),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(3, 3),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(3, 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the parameter values for our model and the computation graph.\n",
    "\n",
    "<img src=\"images/wholeNetwork.png\" width=\"60%\">\n",
    "\n",
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247]])\n",
      "0.bias tensor([ 0.1826, -0.1949, -0.0365])\n",
      "2.weight tensor([[-0.0520,  0.0837, -0.0023],\n",
      "        [ 0.5047,  0.1797, -0.2150],\n",
      "        [-0.3487, -0.0968, -0.2490]])\n",
      "2.bias tensor([-0.1850,  0.0276,  0.3442])\n",
      "4.weight tensor([[ 0.3138, -0.5644,  0.3579],\n",
      "        [ 0.1613,  0.5476,  0.3811],\n",
      "        [-0.5260, -0.5489, -0.2785]])\n",
      "4.bias tensor([ 0.5070, -0.0962,  0.2471])\n",
      "6.weight tensor([[-0.2683,  0.5665, -0.2443],\n",
      "        [ 0.4330,  0.0068, -0.3042]])\n",
      "6.bias tensor([ 0.2968, -0.3065])\n",
      "8.weight tensor([[ 0.2080, -0.2042]])\n",
      "8.bias tensor([-0.0775])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"276pt\" height=\"764pt\"\n",
       " viewBox=\"0.00 0.00 275.99 764.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 760)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-760 271.9873,-760 271.9873,4 -4,4\"/>\n",
       "<!-- 4611468704 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>4611468704</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"178.8208,-20 73.1792,-20 73.1792,0 178.8208,0 178.8208,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"126\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 4611468760 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4611468760</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"177.9785,-76 74.0215,-76 74.0215,-56 177.9785,-56 177.9785,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"126\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 4611468760&#45;&gt;4611468704 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>4611468760&#45;&gt;4611468704</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M126,-55.9883C126,-48.9098 126,-39.1714 126,-30.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"129.5001,-30.3038 126,-20.3039 122.5001,-30.3039 129.5001,-30.3038\"/>\n",
       "</g>\n",
       "<!-- 4611469096 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4611469096</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"64,-144 10,-144 10,-112 64,-112 64,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"37\" y=\"-130.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">8.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"37\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 4611469096&#45;&gt;4611468760 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4611469096&#45;&gt;4611468760</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M60.3837,-111.7102C73.5232,-102.5569 89.8603,-91.176 102.9458,-82.0603\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.2016,-84.7544 111.4062,-76.1664 101.2003,-79.0107 105.2016,-84.7544\"/>\n",
       "</g>\n",
       "<!-- 4611469152 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4611469152</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"170.4641,-138 81.5359,-138 81.5359,-118 170.4641,-118 170.4641,-138\"/>\n",
       "<text text-anchor=\"middle\" x=\"126\" y=\"-124.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TanhBackward</text>\n",
       "</g>\n",
       "<!-- 4611469152&#45;&gt;4611468760 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4611469152&#45;&gt;4611468760</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M126,-117.762C126,-109.185 126,-96.6836 126,-86.1154\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"129.5001,-86.0475 126,-76.0475 122.5001,-86.0476 129.5001,-86.0475\"/>\n",
       "</g>\n",
       "<!-- 4398145720 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4398145720</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"176.9785,-206 73.0215,-206 73.0215,-186 176.9785,-186 176.9785,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-192.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 4398145720&#45;&gt;4611469152 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4398145720&#45;&gt;4611469152</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125.1476,-185.9664C125.2923,-176.1231 125.5172,-160.827 125.6997,-148.4189\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"129.2043,-148.1252 125.8518,-138.0748 122.2051,-148.0222 129.2043,-148.1252\"/>\n",
       "</g>\n",
       "<!-- 4398145608 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4398145608</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-280 0,-280 0,-248 54,-248 54,-280\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-266.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">6.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-254.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (2)</text>\n",
       "</g>\n",
       "<!-- 4398145608&#45;&gt;4398145720 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4398145608&#45;&gt;4398145720</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.2246,-247.8849C65.8387,-237.0507 86.3341,-222.8294 101.9182,-212.0159\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.2318,-214.6707 110.4524,-206.0943 100.2412,-208.9196 104.2318,-214.6707\"/>\n",
       "</g>\n",
       "<!-- 4398145944 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4398145944</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"177.8208,-274 72.1792,-274 72.1792,-254 177.8208,-254 177.8208,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-260.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 4398145944&#45;&gt;4398145720 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4398145944&#45;&gt;4398145720</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125,-253.9664C125,-244.1231 125,-228.827 125,-216.4189\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.5001,-216.0748 125,-206.0748 121.5001,-216.0748 128.5001,-216.0748\"/>\n",
       "</g>\n",
       "<!-- 4398145776 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>4398145776</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"176.9785,-342 73.0215,-342 73.0215,-322 176.9785,-322 176.9785,-342\"/>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-328.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 4398145776&#45;&gt;4398145944 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4398145776&#45;&gt;4398145944</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125,-321.9664C125,-312.1231 125,-296.827 125,-284.4189\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.5001,-284.0748 125,-274.0748 121.5001,-284.0748 128.5001,-284.0748\"/>\n",
       "</g>\n",
       "<!-- 4398030800 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>4398030800</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-416 0,-416 0,-384 54,-384 54,-416\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-402.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">4.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-390.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3)</text>\n",
       "</g>\n",
       "<!-- 4398030800&#45;&gt;4398145776 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>4398030800&#45;&gt;4398145776</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.2246,-383.8849C65.8387,-373.0507 86.3341,-358.8294 101.9182,-348.0159\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.2318,-350.6707 110.4524,-342.0943 100.2412,-344.9196 104.2318,-350.6707\"/>\n",
       "</g>\n",
       "<!-- 4398030352 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>4398030352</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"177.8208,-410 72.1792,-410 72.1792,-390 177.8208,-390 177.8208,-410\"/>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-396.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 4398030352&#45;&gt;4398145776 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>4398030352&#45;&gt;4398145776</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125,-389.9664C125,-380.1231 125,-364.827 125,-352.4189\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.5001,-352.0748 125,-342.0748 121.5001,-352.0748 128.5001,-352.0748\"/>\n",
       "</g>\n",
       "<!-- 4398030296 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>4398030296</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"176.9785,-478 73.0215,-478 73.0215,-458 176.9785,-458 176.9785,-478\"/>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-464.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 4398030296&#45;&gt;4398030352 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>4398030296&#45;&gt;4398030352</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125,-457.9664C125,-448.1231 125,-432.827 125,-420.4189\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.5001,-420.0748 125,-410.0748 121.5001,-420.0748 128.5001,-420.0748\"/>\n",
       "</g>\n",
       "<!-- 4398030464 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>4398030464</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"63,-552 9,-552 9,-520 63,-520 63,-552\"/>\n",
       "<text text-anchor=\"middle\" x=\"36\" y=\"-538.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"36\" y=\"-526.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3)</text>\n",
       "</g>\n",
       "<!-- 4398030464&#45;&gt;4398030296 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>4398030464&#45;&gt;4398030296</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M57.0918,-519.8849C71.1393,-509.152 89.5375,-495.0949 103.64,-484.32\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.9672,-486.9466 111.7884,-478.0943 101.7173,-481.3843 105.9672,-486.9466\"/>\n",
       "</g>\n",
       "<!-- 4398030016 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>4398030016</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"169.4641,-546 80.5359,-546 80.5359,-526 169.4641,-526 169.4641,-546\"/>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-532.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TanhBackward</text>\n",
       "</g>\n",
       "<!-- 4398030016&#45;&gt;4398030296 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>4398030016&#45;&gt;4398030296</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125,-525.9664C125,-516.1231 125,-500.827 125,-488.4189\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.5001,-488.0748 125,-478.0748 121.5001,-488.0748 128.5001,-488.0748\"/>\n",
       "</g>\n",
       "<!-- 4398029848 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>4398029848</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"175.9785,-614 72.0215,-614 72.0215,-594 175.9785,-594 175.9785,-614\"/>\n",
       "<text text-anchor=\"middle\" x=\"124\" y=\"-600.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 4398029848&#45;&gt;4398030016 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>4398029848&#45;&gt;4398030016</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M124.1476,-593.9664C124.2923,-584.1231 124.5172,-568.827 124.6997,-556.4189\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.2043,-556.1252 124.8518,-546.0748 121.2051,-556.0222 128.2043,-556.1252\"/>\n",
       "</g>\n",
       "<!-- 4398028000 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>4398028000</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"111,-688 57,-688 57,-656 111,-656 111,-688\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-674.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">0.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-662.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3)</text>\n",
       "</g>\n",
       "<!-- 4398028000&#45;&gt;4398029848 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>4398028000&#45;&gt;4398029848</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M93.4794,-655.8849C99.2811,-646.0222 106.7339,-633.3524 112.8224,-623.0019\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"115.8904,-624.6894 117.9439,-614.2954 109.8568,-621.1402 115.8904,-624.6894\"/>\n",
       "</g>\n",
       "<!-- 4397388464 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>4397388464</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"200.9746,-682 129.0254,-682 129.0254,-662 200.9746,-662 200.9746,-682\"/>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-668.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 4397388464&#45;&gt;4398029848 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>4397388464&#45;&gt;4398029848</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M158.9503,-661.9664C152.7732,-651.7213 143.0344,-635.5693 135.4043,-622.9145\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"138.2354,-620.8314 130.0745,-614.0748 132.2407,-624.4458 138.2354,-620.8314\"/>\n",
       "</g>\n",
       "<!-- 4397388632 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>4397388632</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"193.6612,-756 136.3388,-756 136.3388,-724 193.6612,-724 193.6612,-756\"/>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-742.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">0.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-730.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3, 4)</text>\n",
       "</g>\n",
       "<!-- 4397388632&#45;&gt;4397388464 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>4397388632&#45;&gt;4397388464</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M165,-723.8849C165,-714.5254 165,-702.6379 165,-692.6036\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.5001,-692.2954 165,-682.2954 161.5001,-692.2954 168.5001,-692.2954\"/>\n",
       "</g>\n",
       "<!-- 4398030408 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>4398030408</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"258.9746,-546 187.0254,-546 187.0254,-526 258.9746,-526 258.9746,-546\"/>\n",
       "<text text-anchor=\"middle\" x=\"223\" y=\"-532.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 4398030408&#45;&gt;4398030296 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>4398030408&#45;&gt;4398030296</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M208.5398,-525.9664C192.5444,-514.8676 166.5582,-496.8363 147.8126,-483.8291\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"149.7308,-480.9001 139.5196,-478.0748 145.7402,-486.6512 149.7308,-480.9001\"/>\n",
       "</g>\n",
       "<!-- 4398028952 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>4398028952</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"251.6612,-620 194.3388,-620 194.3388,-588 251.6612,-588 251.6612,-620\"/>\n",
       "<text text-anchor=\"middle\" x=\"223\" y=\"-606.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"223\" y=\"-594.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3, 3)</text>\n",
       "</g>\n",
       "<!-- 4398028952&#45;&gt;4398030408 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>4398028952&#45;&gt;4398030408</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M223,-587.8849C223,-578.5254 223,-566.6379 223,-556.6036\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"226.5001,-556.2954 223,-546.2954 219.5001,-556.2954 226.5001,-556.2954\"/>\n",
       "</g>\n",
       "<!-- 4398028896 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>4398028896</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"267.9746,-410 196.0254,-410 196.0254,-390 267.9746,-390 267.9746,-410\"/>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-396.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 4398028896&#45;&gt;4398145776 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>4398028896&#45;&gt;4398145776</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M216.2118,-389.9664C198.5895,-378.7671 169.8603,-360.5094 149.3539,-347.4772\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"151.1702,-344.4845 140.853,-342.0748 147.4156,-350.3925 151.1702,-344.4845\"/>\n",
       "</g>\n",
       "<!-- 4398030520 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>4398030520</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"260.6612,-484 203.3388,-484 203.3388,-452 260.6612,-452 260.6612,-484\"/>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-470.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">4.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3, 3)</text>\n",
       "</g>\n",
       "<!-- 4398030520&#45;&gt;4398028896 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>4398030520&#45;&gt;4398028896</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M232,-451.8849C232,-442.5254 232,-430.6379 232,-420.6036\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.5001,-420.2954 232,-410.2954 228.5001,-420.2954 235.5001,-420.2954\"/>\n",
       "</g>\n",
       "<!-- 4398146840 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>4398146840</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"267.9746,-274 196.0254,-274 196.0254,-254 267.9746,-254 267.9746,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-260.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 4398146840&#45;&gt;4398145720 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>4398146840&#45;&gt;4398145720</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M216.2118,-253.9664C198.5895,-242.7671 169.8603,-224.5094 149.3539,-211.4772\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"151.1702,-208.4845 140.853,-206.0748 147.4156,-214.3925 151.1702,-208.4845\"/>\n",
       "</g>\n",
       "<!-- 4398030688 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>4398030688</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"260.6612,-348 203.3388,-348 203.3388,-316 260.6612,-316 260.6612,-348\"/>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-334.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">6.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"232\" y=\"-322.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (2, 3)</text>\n",
       "</g>\n",
       "<!-- 4398030688&#45;&gt;4398146840 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>4398030688&#45;&gt;4398146840</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M232,-315.8849C232,-306.5254 232,-294.6379 232,-284.6036\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.5001,-284.2954 232,-274.2954 228.5001,-284.2954 235.5001,-284.2954\"/>\n",
       "</g>\n",
       "<!-- 4611469208 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>4611469208</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"259.9746,-138 188.0254,-138 188.0254,-118 259.9746,-118 259.9746,-138\"/>\n",
       "<text text-anchor=\"middle\" x=\"224\" y=\"-124.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">TBackward</text>\n",
       "</g>\n",
       "<!-- 4611469208&#45;&gt;4611468760 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>4611469208&#45;&gt;4611468760</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M207.8173,-117.762C192.2622,-107.921 168.5411,-92.9138 150.6388,-81.5878\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"152.2036,-78.4362 141.8816,-76.0475 148.4611,-84.3518 152.2036,-78.4362\"/>\n",
       "</g>\n",
       "<!-- 4398146784 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>4398146784</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"252.6612,-212 195.3388,-212 195.3388,-180 252.6612,-180 252.6612,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"224\" y=\"-198.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">8.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"224\" y=\"-186.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1, 2)</text>\n",
       "</g>\n",
       "<!-- 4398146784&#45;&gt;4611469208 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>4398146784&#45;&gt;4611469208</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M224,-179.8849C224,-170.5254 224,-158.6379 224,-148.6036\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"227.5001,-148.2954 224,-138.2954 220.5001,-148.2954 227.5001,-148.2954\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x112dd6eb8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(model(x), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logic\n",
    "\n",
    "<img src=\"images/benchmarkExample.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y):\n",
    "    # Training Logic\n",
    "    \n",
    "    epochs = 200\n",
    "    learnRate = 0.3\n",
    "    \n",
    "    opt = optim.SGD(params=model.parameters(),lr=learnRate)\n",
    "    counter = 0\n",
    "    startTime = time.time()\n",
    "    \n",
    "    for iter in range(epochs):\n",
    "\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        pred = model(x)\n",
    "\n",
    "        # 3) calculate how much we missed\n",
    "        loss = ((pred - y)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) change those weights\n",
    "        opt.step()\n",
    "\n",
    "        # 6) print our progress every 20 epochs\n",
    "        counter = counter+1\n",
    "        if counter%(epochs/10) == 0:\n",
    "            print(\"Predictions: \",torch.t(pred.data))\n",
    "            print(\"Loss: \",loss.data)\n",
    "            print(\"Runtime: \",time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  tensor([[0.5001, 0.5002, 0.5000, 0.4999, 0.5001, 0.5000, 0.4998, 0.4999, 0.5006,\n",
      "         0.5006, 0.5005, 0.5003, 0.5005, 0.5004, 0.5002, 0.5003]])\n",
      "Loss:  tensor(3.9965)\n",
      "Runtime:  0.025581836700439453\n",
      "Predictions:  tensor([[0.5000, 0.5001, 0.4998, 0.4996, 0.4999, 0.4998, 0.4994, 0.4995, 0.5008,\n",
      "         0.5008, 0.5007, 0.5005, 0.5007, 0.5005, 0.5003, 0.5004]])\n",
      "Loss:  tensor(3.9934)\n",
      "Runtime:  0.03984689712524414\n",
      "Predictions:  tensor([[0.4994, 0.4995, 0.4990, 0.4986, 0.4991, 0.4988, 0.4983, 0.4983, 0.5015,\n",
      "         0.5012, 0.5013, 0.5009, 0.5012, 0.5007, 0.5006, 0.5005]])\n",
      "Loss:  tensor(3.9829)\n",
      "Runtime:  0.05080699920654297\n",
      "Predictions:  tensor([[0.4949, 0.4942, 0.4935, 0.4914, 0.4924, 0.4910, 0.4902, 0.4890, 0.5076,\n",
      "         0.5055, 0.5069, 0.5051, 0.5057, 0.5031, 0.5042, 0.5029]])\n",
      "Loss:  tensor(3.8964)\n",
      "Runtime:  0.06141185760498047\n",
      "Predictions:  tensor([[0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0224,\n",
      "         0.0196, 0.0197, 0.0196, 0.0179, 0.0178, 0.0178, 0.0172]])\n",
      "Loss:  tensor(7.7011)\n",
      "Runtime:  0.07413101196289062\n",
      "Predictions:  tensor([[0.0595, 0.0569, 0.0577, 0.0569, 0.0555, 0.0553, 0.0559, 0.0545, 0.9594,\n",
      "         0.9582, 0.9589, 0.9586, 0.9577, 0.9569, 0.9578, 0.9561]])\n",
      "Loss:  tensor(0.0397)\n",
      "Runtime:  0.0883798599243164\n",
      "Predictions:  tensor([[0.0373, 0.0354, 0.0359, 0.0354, 0.0345, 0.0343, 0.0347, 0.0337, 0.9739,\n",
      "         0.9731, 0.9735, 0.9733, 0.9727, 0.9722, 0.9728, 0.9717]])\n",
      "Loss:  tensor(0.0158)\n",
      "Runtime:  0.10139179229736328\n",
      "Predictions:  tensor([[0.0291, 0.0276, 0.0280, 0.0276, 0.0268, 0.0267, 0.0271, 0.0263, 0.9793,\n",
      "         0.9787, 0.9790, 0.9789, 0.9784, 0.9780, 0.9784, 0.9775]])\n",
      "Loss:  tensor(0.0097)\n",
      "Runtime:  0.11350607872009277\n",
      "Predictions:  tensor([[0.0246, 0.0233, 0.0237, 0.0233, 0.0226, 0.0225, 0.0228, 0.0222, 0.9824,\n",
      "         0.9818, 0.9821, 0.9820, 0.9816, 0.9813, 0.9816, 0.9808]])\n",
      "Loss:  tensor(0.0070)\n",
      "Runtime:  0.12561583518981934\n",
      "Predictions:  tensor([[0.0217, 0.0205, 0.0208, 0.0205, 0.0199, 0.0198, 0.0201, 0.0195, 0.9844,\n",
      "         0.9839, 0.9842, 0.9841, 0.9837, 0.9834, 0.9837, 0.9830]])\n",
      "Loss:  tensor(0.0054)\n",
      "Runtime:  0.13845181465148926\n",
      "\n",
      "\n",
      "NEXT BATCH\n",
      "\n",
      "\n",
      "Predictions:  tensor([[0.0196, 0.0185, 0.0188, 0.0185, 0.0179, 0.0179, 0.0181, 0.0175, 0.9858,\n",
      "         0.9854, 0.9857, 0.9855, 0.9852, 0.9850, 0.9852, 0.9846]])\n",
      "Loss:  tensor(0.0044)\n",
      "Runtime:  0.011893749237060547\n",
      "Predictions:  tensor([[0.0179, 0.0169, 0.0172, 0.0170, 0.0165, 0.0164, 0.0166, 0.0161, 0.9870,\n",
      "         0.9866, 0.9868, 0.9867, 0.9864, 0.9861, 0.9864, 0.9858]])\n",
      "Loss:  tensor(0.0037)\n",
      "Runtime:  0.023904800415039062\n",
      "Predictions:  tensor([[0.0167, 0.0157, 0.0160, 0.0157, 0.0153, 0.0152, 0.0154, 0.0149, 0.9879,\n",
      "         0.9875, 0.9877, 0.9876, 0.9873, 0.9871, 0.9873, 0.9868]])\n",
      "Loss:  tensor(0.0032)\n",
      "Runtime:  0.03611397743225098\n",
      "Predictions:  tensor([[0.0156, 0.0147, 0.0150, 0.0147, 0.0143, 0.0142, 0.0144, 0.0140, 0.9886,\n",
      "         0.9882, 0.9884, 0.9883, 0.9881, 0.9879, 0.9881, 0.9876]])\n",
      "Loss:  tensor(0.0028)\n",
      "Runtime:  0.04829692840576172\n",
      "Predictions:  tensor([[0.0147, 0.0139, 0.0141, 0.0139, 0.0135, 0.0134, 0.0136, 0.0132, 0.9892,\n",
      "         0.9889, 0.9891, 0.9890, 0.9887, 0.9885, 0.9887, 0.9883]])\n",
      "Loss:  tensor(0.0025)\n",
      "Runtime:  0.05984067916870117\n",
      "Predictions:  tensor([[0.0140, 0.0132, 0.0134, 0.0132, 0.0128, 0.0127, 0.0129, 0.0125, 0.9897,\n",
      "         0.9894, 0.9896, 0.9895, 0.9893, 0.9891, 0.9893, 0.9888]])\n",
      "Loss:  tensor(0.0023)\n",
      "Runtime:  0.07321286201477051\n",
      "Predictions:  tensor([[0.0133, 0.0126, 0.0128, 0.0126, 0.0122, 0.0121, 0.0123, 0.0119, 0.9902,\n",
      "         0.9899, 0.9901, 0.9900, 0.9898, 0.9896, 0.9898, 0.9893]])\n",
      "Loss:  tensor(0.0021)\n",
      "Runtime:  0.08447074890136719\n",
      "Predictions:  tensor([[0.0128, 0.0120, 0.0122, 0.0120, 0.0117, 0.0116, 0.0118, 0.0114, 0.9906,\n",
      "         0.9903, 0.9905, 0.9904, 0.9902, 0.9900, 0.9902, 0.9898]])\n",
      "Loss:  tensor(0.0019)\n",
      "Runtime:  0.09744381904602051\n",
      "Predictions:  tensor([[0.0123, 0.0116, 0.0118, 0.0116, 0.0112, 0.0112, 0.0113, 0.0110, 0.9909,\n",
      "         0.9907, 0.9908, 0.9908, 0.9905, 0.9904, 0.9906, 0.9902]])\n",
      "Loss:  tensor(0.0018)\n",
      "Runtime:  0.1075139045715332\n",
      "Predictions:  tensor([[0.0118, 0.0111, 0.0113, 0.0111, 0.0108, 0.0107, 0.0109, 0.0106, 0.9913,\n",
      "         0.9910, 0.9911, 0.9911, 0.9909, 0.9907, 0.9909, 0.9905]])\n",
      "Loss:  tensor(0.0016)\n",
      "Runtime:  0.12292194366455078\n",
      "\n",
      "\n",
      "NEXT BATCH\n",
      "\n",
      "\n",
      "Predictions:  tensor([[0.0114, 0.0108, 0.0109, 0.0108, 0.0104, 0.0104, 0.0105, 0.0102, 0.9915,\n",
      "         0.9913, 0.9914, 0.9914, 0.9912, 0.9910, 0.9912, 0.9908]])\n",
      "Loss:  tensor(0.0015)\n",
      "Runtime:  0.011463165283203125\n",
      "Predictions:  tensor([[0.0110, 0.0104, 0.0106, 0.0104, 0.0101, 0.0100, 0.0102, 0.0099, 0.9918,\n",
      "         0.9916, 0.9917, 0.9916, 0.9914, 0.9913, 0.9915, 0.9911]])\n",
      "Loss:  tensor(0.0014)\n",
      "Runtime:  0.022604942321777344\n",
      "Predictions:  tensor([[0.0107, 0.0101, 0.0103, 0.0101, 0.0098, 0.0097, 0.0099, 0.0096, 0.9920,\n",
      "         0.9918, 0.9919, 0.9919, 0.9917, 0.9916, 0.9917, 0.9914]])\n",
      "Loss:  tensor(0.0013)\n",
      "Runtime:  0.03299713134765625\n",
      "Predictions:  tensor([[0.0104, 0.0098, 0.0100, 0.0098, 0.0095, 0.0095, 0.0096, 0.0093, 0.9923,\n",
      "         0.9920, 0.9922, 0.9921, 0.9919, 0.9918, 0.9919, 0.9916]])\n",
      "Loss:  tensor(0.0013)\n",
      "Runtime:  0.04430198669433594\n",
      "Predictions:  tensor([[0.0101, 0.0095, 0.0097, 0.0095, 0.0092, 0.0092, 0.0093, 0.0090, 0.9925,\n",
      "         0.9922, 0.9924, 0.9923, 0.9921, 0.9920, 0.9921, 0.9918]])\n",
      "Loss:  tensor(0.0012)\n",
      "Runtime:  0.05415487289428711\n",
      "Predictions:  tensor([[0.0098, 0.0093, 0.0094, 0.0093, 0.0090, 0.0090, 0.0091, 0.0088, 0.9927,\n",
      "         0.9924, 0.9926, 0.9925, 0.9923, 0.9922, 0.9923, 0.9920]])\n",
      "Loss:  tensor(0.0011)\n",
      "Runtime:  0.06506490707397461\n",
      "Predictions:  tensor([[0.0096, 0.0091, 0.0092, 0.0091, 0.0088, 0.0087, 0.0089, 0.0086, 0.9928,\n",
      "         0.9926, 0.9927, 0.9927, 0.9925, 0.9924, 0.9925, 0.9922]])\n",
      "Loss:  tensor(0.0011)\n",
      "Runtime:  0.0759429931640625\n",
      "Predictions:  tensor([[0.0094, 0.0088, 0.0090, 0.0088, 0.0086, 0.0085, 0.0087, 0.0084, 0.9930,\n",
      "         0.9928, 0.9929, 0.9928, 0.9927, 0.9926, 0.9927, 0.9924]])\n",
      "Loss:  tensor(0.0010)\n",
      "Runtime:  0.08546090126037598\n",
      "Predictions:  tensor([[0.0092, 0.0086, 0.0088, 0.0086, 0.0084, 0.0083, 0.0085, 0.0082, 0.9931,\n",
      "         0.9929, 0.9930, 0.9930, 0.9928, 0.9927, 0.9928, 0.9925]])\n",
      "Loss:  tensor(0.0010)\n",
      "Runtime:  0.09664511680603027\n",
      "Predictions:  tensor([[0.0090, 0.0085, 0.0086, 0.0085, 0.0082, 0.0082, 0.0083, 0.0080, 0.9933,\n",
      "         0.9931, 0.9932, 0.9931, 0.9930, 0.9929, 0.9930, 0.9927]])\n",
      "Loss:  tensor(0.0010)\n",
      "Runtime:  0.10778093338012695\n",
      "\n",
      "\n",
      "Final Predictions: tensor([[0.0090, 0.0084, 0.0086, 0.0085, 0.0082, 0.0081, 0.0083, 0.0080, 0.9933,\n",
      "         0.9931, 0.9932, 0.9932, 0.9930, 0.9929, 0.9930, 0.9927]],\n",
      "       grad_fn=<TBackward>)\n"
     ]
    }
   ],
   "source": [
    "train(x1, y1)\n",
    "print(\"\\n\\nNEXT BATCH\\n\\n\")\n",
    "train(x2, y2)\n",
    "print(\"\\n\\nNEXT BATCH\\n\\n\")\n",
    "train(x3, y3)\n",
    "print(\"\\n\\nFinal Predictions:\", torch.t(model(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.1 - A Distributed Training Example\n",
    "\n",
    "We will train a splitNN model that has been distributed to three different hosts. One host, Alice, is the data subject. Alice has the labelled data and will also be the custodian of the network start and end segments. Claire and Bob are worker hosts. They will feed the activation signals from the start of the chain forward until it reaches alices end layer. They will do the reverse with gradients in the backpropogation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.1 - Set up environmental variables\n",
    "\n",
    "Here we will import our required libraries and initialise our model segments and data. We will need;\n",
    "\n",
    "<img src=\"images/distributed.png\" width=\"50%\">\n",
    "\n",
    "- A dummy distributed dataset\n",
    "- 5 model segments\n",
    "- 3 Virtual Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "x = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "y = torch.tensor([[0],[0],[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[1],[1.]])\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# Five Linked Models\n",
    "model1 = nn.Sequential(\n",
    "            nn.Linear(4, 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "model2 = nn.Sequential(\n",
    "            nn.Linear(3, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "model3 = nn.Sequential(\n",
    "            nn.Linear(3, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "model4 = nn.Sequential(\n",
    "            nn.Linear(3, 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "model5 = nn.Sequential(\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "claire = sy.VirtualWorker(hook, id=\"claire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final predictions are shown above, we can compare this with the output of the same 'split' neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.2 - Set Environmental environmental variables to Starting Locations\n",
    "\n",
    "In this example, Alice is the worker with the data and labels. Bob and Claire are intermediary hosts in the chain. Alice has the start and end model segments. Bob and Claire have intermediary segments.\n",
    "\n",
    "We send the models and data to their respective hosts and store the pointers in associative arrays; the Model Chain (MC) and the xy Chain (xyC). These contain the locations of the data, but no actual values. These are the only necessary parameters for coordinating this learning process. A summary of this is seen below\n",
    "\n",
    "<img src=\"images/Parameters.png\" width=\"50%\">\n",
    "\n",
    "In this experiment, the models and data are initialised locally and then distributed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Send Model Segmemnts to starting locations\n",
    "model1 = model1.send(alice)\n",
    "model2 = model2.send(alice)\n",
    "model3 = model3.send(bob)\n",
    "model4 = model4.send(claire)\n",
    "model5 = model5.send(alice)\n",
    "\n",
    "# Distribute the input data\n",
    "x1 = x.send(alice)\n",
    "x2 = x.send(bob)\n",
    "x3 = x.send(claire)\n",
    "\n",
    "# Distribute the data labels\n",
    "y1 = y.send(alice)\n",
    "y2 = y.send(bob)\n",
    "y3 = y.send(claire)\n",
    "\n",
    "# Add these values to lists for use in training\n",
    "xChain = [x1,x2,x3]\n",
    "yChain = [y1,y2,y3]\n",
    "\n",
    "xyChain = [xChain,yChain]\n",
    "\n",
    "dataChain = [xChain, yChain]\n",
    "# Add these models to a list to be used during training\n",
    "modelChain = [model1, model2, model3, model4, model5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.3 - Forward Propogation\n",
    "\n",
    "We will need to define the logic of forward and backward propogation. \n",
    "\n",
    "Forward propogation feeds the input data into Alice's segment at the beginning of the chain. Alice then sends her activation signal to the location of the next model in the chain. This model propogates this activation and sends it onward to the location of the next segment. The signal will eventually reach alice's end segment to perform a prediction. We store pointers to the activations of each layer using the Activation Chain (AC). This allows us to retrieve the values when processing gradients. When the activations have fully propogated the MC, the method returns the resultant AC for use in the backpropogation function. \n",
    "\n",
    "<img src=\"images/activationchain.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropogate(modelChain, x):\n",
    "\n",
    "    activationChain = []\n",
    "    \n",
    "    activationChain.append(modelChain[0](x))\n",
    "    activationChain[0] = activationChain[0].get().send(modelChain[1].__getitem__(0).weight[0].location)\n",
    "    \n",
    "    i = 1\n",
    "    for iter in range(len(modelChain)-2):\n",
    "        activationChain.append(modelChain[i](activationChain[i-1]))\n",
    "        activationChain[i] = activationChain[i].get().send(modelChain[i+1].__getitem__(0).weight[0].location)\n",
    "        i += 1\n",
    "\n",
    "    activationChain.append(modelChain[len(modelChain)-1](activationChain[i-1]))\n",
    "    \n",
    "    return activationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.4 - Backward Propogation\n",
    "\n",
    "The backpropogation function takes the MC, xyC and AC as input parameters.\n",
    "\n",
    "<img src=\"images/backpropParams.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "First the backpropogation algorithm computes the loss on Alice's prediction. We use <b>**** what seems to be**** </b> the sum of squared error as our loss function.\n",
    "\n",
    "<img src=\"images/loss.png\" width=\"100%\">\n",
    "\n",
    "We then calculate the gradients for the parameters of the end segment using the chain rule.\n",
    "\n",
    "<img src=\"images/chainRule.png\" width=\"40%\">\n",
    "\n",
    "This is done automatically for the layers in each segment but we have to recalculate loss for each model segment during the backpropogation phase.\n",
    "\n",
    "<img src=\"images/intermediateLoss.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Each segment feeds the gradients of their activation function back to the segment behind them and updates their weights w.r.t these gradients. This layer computes it's loss by dot joining the orignal activation signal and it's new gradient. The sum of the result is used to feed back error down the line. After each segment is complete, the optimiser for that model updates. The process is repeated until the segment at the beginning of the chain is reached and alice updates the gradients on her beginning segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropogate(modelChain, optChain, activationChain, x, y):\n",
    "        \n",
    "#   Destroy pre-existing gradient of final layer\n",
    "    optChain[len(optChain)-1].zero_grad()\n",
    "    \n",
    "#     TODO: I DONT KNOW IF ITS NECESSARY TO RECONSTRUCT GRAPH AT EACH LAYER.\n",
    "#         CAN COMPUTATION GRAPHS EXTEND OVER MODELS ON MULTIPLE HOSTS? AT THE MOMENT\n",
    "#         I'M JUST FLUSHING THE OLD COMPUTATION GRAPH AND GENERATING A NEW ONE FOR\n",
    "#         THE PURPOSE OF FEEDING BACK ERROR. THIS IS HIGHLY INEFFICIENT.\n",
    "#   Constructs computation graph for final layer\n",
    "    activationChain[len(activationChain)-2].requires_grad = True\n",
    "    activationChain[len(activationChain)-1] = modelChain[len(modelChain)-1](activationChain[len(activationChain)-2])\n",
    "    \n",
    "#     TODO: LOOKS LIKE JUST SQUARED ERROR, NOT MEAN SQUARED. \n",
    "#         NOT SURE IF I HAVE THE RIGHT LOSS EQUATION. COULD BE\n",
    "#         THAT THIS IS DONE AS PART OF THE .SUM() FUNCTION THOUGH?\n",
    "#         WHEN I ADD THE /n PART IT DOESN'T LEARN SO WELL..\n",
    "#   Calculates Loss\n",
    "#     loss = (((activationChain[len(activationChain)-1] - y)**2).sum())/(len(activationChain))\n",
    "    loss = (((activationChain[len(activationChain)-1] - y)**2).sum())\n",
    "\n",
    "    \n",
    "#   Calculates Gradients\n",
    "    loss.backward()\n",
    "        \n",
    "#     for iter in range(len(activationChain)):\n",
    "#         print(activationChain[iter].location)\n",
    "    \n",
    "    \n",
    "#   End layer sends the gradient of the activation signal back to the layer behind\n",
    "    DofA = activationChain[len(activationChain)-2].grad.clone().get().send(modelChain[len(modelChain)-2].__getitem__(0).weight[0].location)\n",
    "    \n",
    "#   End layer updates weights\n",
    "    optChain[len(optChain)-1].step()\n",
    "\n",
    "#   Compute Intermediary Layers\n",
    "    for iter in range(len(modelChain)-1, 1, -1): \n",
    "        optChain[iter-1].zero_grad()\n",
    "        activationChain[len(activationChain)-iter-1].requires_grad = True\n",
    "        activationChain[iter-1] = modelChain[iter-1](activationChain[iter-2]) \n",
    "        feedbackError = torch.matmul(torch.t(activationChain[iter-1]), DofA).sum()\n",
    "        feedbackError.backward()\n",
    "        DofA = activationChain[iter-2].grad.clone().get().send(modelChain[iter-2].__getitem__(0).weight[0].location)\n",
    "        optChain[iter-1].step()\n",
    "\n",
    "#   Compute Final Layer \n",
    "    optChain[0].zero_grad()\n",
    "    activationChain[0] = modelChain[0](x) \n",
    "    feedbackError = torch.matmul(torch.t(activationChain[0]), DofA).sum()\n",
    "    feedbackError.backward()\n",
    "    optChain[0].step()\n",
    "        \n",
    "    return activationChain[len(activationChain)-1], loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.5 - Run Training Logic\n",
    "\n",
    "Now we will run the training process over 200 epochs for each data owner. Every 20 epochs we will print our progress. The front and end sections of the model will be swapped between data owners training each individual batch.\n",
    "\n",
    "<img src=\"images/BatchFlow.png\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnonymousSplitNNtrain(modelChain, xyChain):\n",
    "    \n",
    "    #   Variables for performance metrics\n",
    "    startTime = time.time()\n",
    "    epochs = 400\n",
    "    learnRate = 0.2\n",
    "    counter = 0\n",
    "    \n",
    "    #   Create optimisers for each segment and link to their segment    \n",
    "    optChain = [0]*len(modelChain)\n",
    "    for iter in range(len(modelChain)):\n",
    "        optChain[iter] = optim.SGD(params=modelChain[iter].parameters(),lr=learnRate)\n",
    "    \n",
    "    \n",
    "    for i in range(len(xyChain[0])):\n",
    "        \n",
    "#       Begin work on current data subjecta\n",
    "        x = xyChain[0][i]\n",
    "        y = xyChain[1][i]\n",
    "        \n",
    "        for iter in range(epochs):\n",
    "        #   Forward propogate through network until final layer is reached\n",
    "            activationChain = forwardPropogate(modelChain, x)\n",
    "            \n",
    "#             DEBUGGING\n",
    "#             for iter in range(len(activationChain)):\n",
    "#                 print(activationChain[iter].location)\n",
    "            \n",
    "        #   Backward propogate\n",
    "            predictions, loss = backwardPropogate(modelChain, optChain, activationChain, x, y)\n",
    "\n",
    "            counter = counter+1\n",
    "            if counter%(epochs/10) == 0:\n",
    "                print(\"Epoch: \",counter)\n",
    "                print(\"Predictions: \",torch.t(predictions).get().data)\n",
    "                print(\"Loss: \",loss.get().data)\n",
    "                print(\"Runtime: \",time.time() - startTime)\n",
    "    \n",
    "        counter = 0\n",
    "        \n",
    "#       If we are not at the end of the data owner chain send perimeter segments to next data owner\n",
    "        if i < len(xyChain[0])-1:\n",
    "            modelChain[0].get().send(xyChain[0][i+1].location)\n",
    "            modelChain[len(modelChain)-1].get().send(xyChain[0][i+1].location)      \n",
    "            \n",
    "\n",
    "            print(\"\\nNEXT DATA OWNER\\n\")\n",
    "            print(\"MODEL CHAIN LOCATIONS\\n\")\n",
    "            for iter in range(len(modelChain)):\n",
    "                print(modelChain[iter].__getitem__(0).weight[0].location)\n",
    "                \n",
    "            print(\"\\n\\n\")\n",
    "    \n",
    "#     Aggregate models back to researcher\n",
    "    for i in range(len(modelChain)):\n",
    "        modelChain[i] = modelChain[i].get()\n",
    "    \n",
    "#     Perform predictions with update weights\n",
    "    x = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "    for i in range(len(modelChain)):\n",
    "        x = modelChain[i](x)\n",
    "        \n",
    "    print(\"\\n\\nFinal Predictions:\", torch.t(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  40\n",
      "Predictions:  tensor([[0.5003, 0.5004, 0.5002, 0.5001, 0.5003, 0.5002, 0.4999, 0.5001, 0.5006,\n",
      "         0.5006, 0.5005, 0.5004, 0.5006, 0.5005, 0.5003, 0.5004]])\n",
      "Loss:  tensor(3.9975)\n",
      "Runtime:  1.8710930347442627\n",
      "Epoch:  80\n",
      "Predictions:  tensor([[0.5003, 0.5004, 0.5001, 0.5000, 0.5002, 0.5002, 0.4999, 0.5000, 0.5006,\n",
      "         0.5007, 0.5005, 0.5004, 0.5006, 0.5005, 0.5003, 0.5004]])\n",
      "Loss:  tensor(3.9971)\n",
      "Runtime:  3.486536979675293\n",
      "Epoch:  120\n",
      "Predictions:  tensor([[0.5002, 0.5004, 0.5001, 0.5000, 0.5002, 0.5001, 0.4998, 0.5000, 0.5007,\n",
      "         0.5007, 0.5005, 0.5004, 0.5006, 0.5005, 0.5003, 0.5004]])\n",
      "Loss:  tensor(3.9966)\n",
      "Runtime:  5.065863847732544\n",
      "Epoch:  160\n",
      "Predictions:  tensor([[0.5002, 0.5004, 0.5001, 0.4999, 0.5002, 0.5001, 0.4997, 0.4999, 0.5007,\n",
      "         0.5008, 0.5006, 0.5004, 0.5007, 0.5005, 0.5003, 0.5004]])\n",
      "Loss:  tensor(3.9962)\n",
      "Runtime:  6.65485405921936\n",
      "Epoch:  200\n",
      "Predictions:  tensor([[0.5002, 0.5004, 0.5000, 0.4998, 0.5002, 0.5000, 0.4996, 0.4998, 0.5008,\n",
      "         0.5008, 0.5006, 0.5004, 0.5007, 0.5005, 0.5002, 0.5004]])\n",
      "Loss:  tensor(3.9957)\n",
      "Runtime:  8.357669115066528\n",
      "Epoch:  240\n",
      "Predictions:  tensor([[0.5002, 0.5003, 0.5000, 0.4997, 0.5001, 0.5000, 0.4995, 0.4997, 0.5008,\n",
      "         0.5008, 0.5006, 0.5004, 0.5007, 0.5005, 0.5002, 0.5003]])\n",
      "Loss:  tensor(3.9951)\n",
      "Runtime:  10.032424926757812\n",
      "Epoch:  280\n",
      "Predictions:  tensor([[0.5001, 0.5003, 0.4999, 0.4996, 0.5000, 0.4999, 0.4994, 0.4996, 0.5008,\n",
      "         0.5008, 0.5006, 0.5004, 0.5007, 0.5005, 0.5002, 0.5003]])\n",
      "Loss:  tensor(3.9944)\n",
      "Runtime:  11.675100088119507\n",
      "Epoch:  320\n",
      "Predictions:  tensor([[0.5000, 0.5002, 0.4998, 0.4995, 0.5000, 0.4997, 0.4992, 0.4994, 0.5008,\n",
      "         0.5009, 0.5006, 0.5003, 0.5008, 0.5004, 0.5001, 0.5002]])\n",
      "Loss:  tensor(3.9936)\n",
      "Runtime:  13.483277082443237\n",
      "Epoch:  360\n",
      "Predictions:  tensor([[0.4999, 0.5001, 0.4996, 0.4993, 0.4998, 0.4996, 0.4990, 0.4992, 0.5009,\n",
      "         0.5009, 0.5006, 0.5003, 0.5008, 0.5004, 0.5000, 0.5002]])\n",
      "Loss:  tensor(3.9926)\n",
      "Runtime:  15.121851205825806\n",
      "Epoch:  400\n",
      "Predictions:  tensor([[0.4998, 0.5000, 0.4995, 0.4990, 0.4997, 0.4993, 0.4987, 0.4989, 0.5009,\n",
      "         0.5009, 0.5006, 0.5002, 0.5008, 0.5003, 0.4999, 0.5001]])\n",
      "Loss:  tensor(3.9913)\n",
      "Runtime:  16.9287211894989\n",
      "\n",
      "NEXT DATA OWNER\n",
      "\n",
      "MODEL CHAIN LOCATIONS\n",
      "\n",
      "<VirtualWorker id:bob #tensors:9>\n",
      "<VirtualWorker id:alice #tensors:7>\n",
      "<VirtualWorker id:bob #tensors:9>\n",
      "<VirtualWorker id:claire #tensors:5>\n",
      "<VirtualWorker id:bob #tensors:9>\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "Predictions:  tensor([[0.4996, 0.4999, 0.4993, 0.4987, 0.4995, 0.4991, 0.4983, 0.4986, 0.5010,\n",
      "         0.5010, 0.5007, 0.5001, 0.5008, 0.5002, 0.4998, 0.5000]])\n",
      "Loss:  tensor(3.9895)\n",
      "Runtime:  18.708224058151245\n",
      "Epoch:  80\n",
      "Predictions:  tensor([[0.4994, 0.4997, 0.4990, 0.4983, 0.4993, 0.4987, 0.4979, 0.4982, 0.5011,\n",
      "         0.5011, 0.5008, 0.5000, 0.5009, 0.5001, 0.4997, 0.4998]])\n",
      "Loss:  tensor(3.9869)\n",
      "Runtime:  20.43476104736328\n",
      "Epoch:  120\n",
      "Predictions:  tensor([[0.4992, 0.4995, 0.4987, 0.4977, 0.4989, 0.4982, 0.4972, 0.4974, 0.5015,\n",
      "         0.5013, 0.5011, 0.5001, 0.5012, 0.5001, 0.4996, 0.4997]])\n",
      "Loss:  tensor(3.9822)\n",
      "Runtime:  22.022037982940674\n",
      "Epoch:  160\n",
      "Predictions:  tensor([[0.4985, 0.4986, 0.4979, 0.4961, 0.4979, 0.4966, 0.4955, 0.4956, 0.5027,\n",
      "         0.5020, 0.5022, 0.5005, 0.5019, 0.5001, 0.4999, 0.4998]])\n",
      "Loss:  tensor(3.9678)\n",
      "Runtime:  23.57692313194275\n",
      "Epoch:  200\n",
      "Predictions:  tensor([[0.1227, 0.1206, 0.1206, 0.1206, 0.1193, 0.1193, 0.1193, 0.1185, 0.8740,\n",
      "         0.8735, 0.8736, 0.8736, 0.8729, 0.8729, 0.8729, 0.8719]])\n",
      "Loss:  tensor(0.2441)\n",
      "Runtime:  25.21806502342224\n",
      "Epoch:  240\n",
      "Predictions:  tensor([[0.0322, 0.0320, 0.0320, 0.0320, 0.0318, 0.0318, 0.0318, 0.0317, 0.9668,\n",
      "         0.9667, 0.9667, 0.9667, 0.9666, 0.9666, 0.9666, 0.9664]])\n",
      "Loss:  tensor(0.0170)\n",
      "Runtime:  26.844994068145752\n",
      "Epoch:  280\n",
      "Predictions:  tensor([[0.0228, 0.0227, 0.0227, 0.0227, 0.0226, 0.0226, 0.0226, 0.0225, 0.9768,\n",
      "         0.9767, 0.9767, 0.9767, 0.9766, 0.9766, 0.9766, 0.9765]])\n",
      "Loss:  tensor(0.0084)\n",
      "Runtime:  28.4738290309906\n",
      "Epoch:  320\n",
      "Predictions:  tensor([[0.0186, 0.0185, 0.0185, 0.0185, 0.0184, 0.0184, 0.0184, 0.0184, 0.9812,\n",
      "         0.9811, 0.9811, 0.9811, 0.9811, 0.9811, 0.9811, 0.9810]])\n",
      "Loss:  tensor(0.0056)\n",
      "Runtime:  30.121551990509033\n",
      "Epoch:  360\n",
      "Predictions:  tensor([[0.0161, 0.0160, 0.0160, 0.0160, 0.0159, 0.0159, 0.0159, 0.0159, 0.9838,\n",
      "         0.9838, 0.9838, 0.9838, 0.9837, 0.9837, 0.9837, 0.9837]])\n",
      "Loss:  tensor(0.0042)\n",
      "Runtime:  31.786412954330444\n",
      "Epoch:  400\n",
      "Predictions:  tensor([[0.0144, 0.0143, 0.0143, 0.0143, 0.0142, 0.0142, 0.0142, 0.0142, 0.9856,\n",
      "         0.9856, 0.9856, 0.9856, 0.9855, 0.9855, 0.9855, 0.9855]])\n",
      "Loss:  tensor(0.0033)\n",
      "Runtime:  33.384098052978516\n",
      "\n",
      "NEXT DATA OWNER\n",
      "\n",
      "MODEL CHAIN LOCATIONS\n",
      "\n",
      "<VirtualWorker id:claire #tensors:9>\n",
      "<VirtualWorker id:alice #tensors:5>\n",
      "<VirtualWorker id:bob #tensors:7>\n",
      "<VirtualWorker id:claire #tensors:9>\n",
      "<VirtualWorker id:claire #tensors:9>\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "Predictions:  tensor([[0.0131, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0129, 0.9869,\n",
      "         0.9869, 0.9869, 0.9869, 0.9868, 0.9868, 0.9868, 0.9868]])\n",
      "Loss:  tensor(0.0027)\n",
      "Runtime:  35.186342000961304\n",
      "Epoch:  80\n",
      "Predictions:  tensor([[0.0121, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.9879,\n",
      "         0.9879, 0.9879, 0.9879, 0.9878, 0.9878, 0.9878, 0.9878]])\n",
      "Loss:  tensor(0.0023)\n",
      "Runtime:  36.911097049713135\n",
      "Epoch:  120\n",
      "Predictions:  tensor([[0.0113, 0.0113, 0.0113, 0.0113, 0.0112, 0.0112, 0.0112, 0.0112, 0.9887,\n",
      "         0.9887, 0.9887, 0.9887, 0.9887, 0.9887, 0.9887, 0.9886]])\n",
      "Loss:  tensor(0.0020)\n",
      "Runtime:  38.815593957901\n",
      "Epoch:  160\n",
      "Predictions:  tensor([[0.0107, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.9894,\n",
      "         0.9894, 0.9894, 0.9894, 0.9893, 0.9893, 0.9893, 0.9893]])\n",
      "Loss:  tensor(0.0018)\n",
      "Runtime:  40.68792009353638\n",
      "Epoch:  200\n",
      "Predictions:  tensor([[0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.9899,\n",
      "         0.9899, 0.9899, 0.9899, 0.9899, 0.9899, 0.9899, 0.9899]])\n",
      "Loss:  tensor(0.0016)\n",
      "Runtime:  42.35374212265015\n",
      "Epoch:  240\n",
      "Predictions:  tensor([[0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0095, 0.9904,\n",
      "         0.9904, 0.9904, 0.9904, 0.9904, 0.9904, 0.9904, 0.9904]])\n",
      "Loss:  tensor(0.0015)\n",
      "Runtime:  44.111714124679565\n",
      "Epoch:  280\n",
      "Predictions:  tensor([[0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.9908,\n",
      "         0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908, 0.9908]])\n",
      "Loss:  tensor(0.0013)\n",
      "Runtime:  45.782774925231934\n",
      "Epoch:  320\n",
      "Predictions:  tensor([[0.0089, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.0088, 0.9912,\n",
      "         0.9912, 0.9912, 0.9912, 0.9912, 0.9912, 0.9912, 0.9912]])\n",
      "Loss:  tensor(0.0012)\n",
      "Runtime:  47.46677803993225\n",
      "Epoch:  360\n",
      "Predictions:  tensor([[0.0085, 0.0085, 0.0085, 0.0085, 0.0085, 0.0085, 0.0085, 0.0085, 0.9915,\n",
      "         0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915, 0.9915]])\n",
      "Loss:  tensor(0.0012)\n",
      "Runtime:  49.10512900352478\n",
      "Epoch:  400\n",
      "Predictions:  tensor([[0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.9918,\n",
      "         0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918]])\n",
      "Loss:  tensor(0.0011)\n",
      "Runtime:  50.75215005874634\n",
      "\n",
      "\n",
      "Final Predictions: tensor([[0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.9919,\n",
      "         0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918, 0.9918]],\n",
      "       grad_fn=<TBackward>)\n"
     ]
    }
   ],
   "source": [
    "AnonymousSplitNNtrain(modelChain, xyChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Figure out whether we need to reconstruct computation graph at each layer\n",
    "- Implement .move() instead of .get().send()\n",
    "    - Should move data directly between owners\n",
    "- Connect the model to the <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST dataset</a> using <a href=\"https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%208%20-%20Introduction%20to%20Plans.ipynb\"> this </a> and <a href=\"https://github.com/blockpass-identity-lab/PySyft/blob/dev/examples/tutorials/Part%206%20-%20Federated%20Learning%20on%20MNIST%20using%20a%20CNN.ipynb\"> this </a> tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
