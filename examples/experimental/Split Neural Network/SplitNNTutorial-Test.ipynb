{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.1 - A Distributed Training Example\n",
    "\n",
    "We will train a splitNN model that has been distributed to three different hosts. One host, Alice, is the data subject. Alice has the labelled data and will also be the custodian of the network start and end segments. Claire and Bob are worker hosts. They will feed the activation signals from the start of the chain forward until it reaches alices end layer. They will do the reverse with gradients in the backpropogation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.1 - Set up environmental variables\n",
    "\n",
    "Here we will import our required libraries and initialise our model segments and data. We will need;\n",
    "\n",
    "<img src=\"images/distributed.png\" width=\"50%\">\n",
    "\n",
    "- A dummy distributed dataset\n",
    "- 5 model segments\n",
    "- 3 Virtual Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import syft as sy\n",
    "import time\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "#from torchviz import make_dot, make_dot_from_trace\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def location(self):\n",
    "    m = self.__getitem__(0)\n",
    "    w = m.weight[0]\n",
    "    return w.location\n",
    "\n",
    "nn.Sequential.location = location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "x = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "y = torch.tensor([[0],[0],[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[1],[1.]])\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define 5 chained models\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(4, 3),\n",
    "        nn.Tanh()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(3, 3),\n",
    "        nn.Sigmoid()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(3, 3),\n",
    "        nn.Sigmoid()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(3, 2),\n",
    "        nn.Tanh()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(2, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "]\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "claire = sy.VirtualWorker(hook, id=\"claire\")\n",
    "workers = alice, bob, claire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final predictions are shown above, we can compare this with the output of the same 'split' neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.2 - Send Variables to Starting Locations\n",
    "\n",
    "In this example, Alice is the worker with the data and labels. Bob and Claire are intermediary hosts in the chain. Alice has the start and end model segments. Bob and Claire have intermediary segments.\n",
    "\n",
    "We send the models and data to their respective hosts and store the pointers in associative arrays; the Model Chain (MC) and the xy Chain (xyC). These contain the locations of the data, but no actual values. These are the only necessary parameters for coordinating this learning process. A summary of this is seen below\n",
    "\n",
    "<img src=\"images/Parameters.png\" width=\"50%\">\n",
    "\n",
    "In this experiment, the models and data are initialised locally and then distributed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Send Model Segments to starting locations\n",
    "model_locations = [alice, alice, bob, claire, alice]\n",
    "\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)\n",
    "\n",
    "# Create a remote copy of the dataset for each worker\n",
    "datasets = [\n",
    "    sy.BaseDataset(x.send(worker), y.send(worker))\n",
    "    for worker in (alice, bob, claire)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(models, x):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    # First: provide x as input\n",
    "    inputs.append(x)\n",
    "    outputs.append(models[0](x))    \n",
    "    \n",
    "    # Set the next input as the output of the previous model and move if necessary\n",
    "    next_input = outputs[-1].clone()\n",
    "    if next_input.location != models[1].location:\n",
    "        next_input.move(models[1].location)\n",
    "        \n",
    "    for i in range(1, len(models)-1):\n",
    "        inputs.append(next_input)\n",
    "        outputs.append(models[i](next_input))\n",
    "        next_input = outputs[-1].clone()\n",
    "        \n",
    "        if next_input.location != models[i+1].location:\n",
    "            next_input.move(models[i+1].location)\n",
    " \n",
    "    # Last: don't move the result to the next location\n",
    "    inputs.append(next_input)\n",
    "    outputs.append(models[len(models)-1](next_input))\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 1.],\n",
      "        [0., 1., 0., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[ 0.1806, -0.1924, -0.0364],\n",
      "        [ 0.4138, -0.5821,  0.0330],\n",
      "        [-0.0381,  0.1046, -0.0974],\n",
      "        [ 0.0855, -0.2892,  0.1019],\n",
      "        [ 0.2161, -0.3502, -0.0282],\n",
      "        [ 0.3304, -0.6460,  0.1701],\n",
      "        [-0.1342,  0.0021,  0.0410],\n",
      "        [ 0.1220, -0.4370,  0.1101],\n",
      "        [ 0.3947,  0.0595, -0.0118],\n",
      "        [ 0.5882, -0.3895,  0.0577],\n",
      "        [ 0.1941,  0.3447, -0.0729],\n",
      "        [ 0.3098, -0.0433,  0.1262],\n",
      "        [ 0.4254, -0.1109, -0.0035],\n",
      "        [ 0.5212, -0.4731,  0.1939],\n",
      "        [ 0.0993,  0.2510,  0.0656],\n",
      "        [ 0.3428, -0.2110,  0.1344]], requires_grad=True)\n",
      "tensor([[0.4476, 0.5230, 0.5766],\n",
      "        [0.4365, 0.5312, 0.5617],\n",
      "        [0.4566, 0.5120, 0.5918],\n",
      "        [0.4467, 0.4992, 0.5786],\n",
      "        [0.4439, 0.5199, 0.5768],\n",
      "        [0.4362, 0.5104, 0.5620],\n",
      "        [0.4556, 0.4879, 0.5940],\n",
      "        [0.4432, 0.4968, 0.5785],\n",
      "        [0.4500, 0.5597, 0.5508],\n",
      "        [0.4382, 0.5602, 0.5405],\n",
      "        [0.4586, 0.5506, 0.5650],\n",
      "        [0.4489, 0.5372, 0.5520],\n",
      "        [0.4461, 0.5556, 0.5517],\n",
      "        [0.4373, 0.5409, 0.5399],\n",
      "        [0.4578, 0.5272, 0.5668],\n",
      "        [0.4450, 0.5334, 0.5527]], requires_grad=True)\n",
      "tensor([[0.6361, 0.6183, 0.3927],\n",
      "        [0.6330, 0.6176, 0.3940],\n",
      "        [0.6395, 0.6186, 0.3920],\n",
      "        [0.6393, 0.6153, 0.3958],\n",
      "        [0.6363, 0.6177, 0.3935],\n",
      "        [0.6357, 0.6149, 0.3967],\n",
      "        [0.6427, 0.6156, 0.3951],\n",
      "        [0.6394, 0.6149, 0.3966],\n",
      "        [0.6293, 0.6208, 0.3893],\n",
      "        [0.6276, 0.6195, 0.3914],\n",
      "        [0.6324, 0.6212, 0.3885],\n",
      "        [0.6323, 0.6179, 0.3923],\n",
      "        [0.6297, 0.6202, 0.3903],\n",
      "        [0.6300, 0.6169, 0.3941],\n",
      "        [0.6355, 0.6183, 0.3915],\n",
      "        [0.6326, 0.6174, 0.3932]], requires_grad=True)\n",
      "tensor([[ 0.3631, -0.1452],\n",
      "        [ 0.3632, -0.1469],\n",
      "        [ 0.3626, -0.1436],\n",
      "        [ 0.3602, -0.1448],\n",
      "        [ 0.3626, -0.1454],\n",
      "        [ 0.3606, -0.1466],\n",
      "        [ 0.3597, -0.1432],\n",
      "        [ 0.3598, -0.1450],\n",
      "        [ 0.3666, -0.1471],\n",
      "        [ 0.3659, -0.1485],\n",
      "        [ 0.3663, -0.1455],\n",
      "        [ 0.3639, -0.1467],\n",
      "        [ 0.3660, -0.1472],\n",
      "        [ 0.3635, -0.1482],\n",
      "        [ 0.3635, -0.1451],\n",
      "        [ 0.3633, -0.1469]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = forward(models, datasets[0].data)\n",
    "\n",
    "for i in inputs:\n",
    "    print(i.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(models, optimizers, segment_inputs, segment_outputs, dataset):\n",
    "    data, targets = dataset.data, dataset.targets\n",
    "        \n",
    "    # Destroy pre-existing gradient of final layer\n",
    "    optimizers[len(optimizers)-1].zero_grad()\n",
    "       \n",
    "    loss = (((segment_outputs[-1] - targets)**2).sum())\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # End layer sends the gradient of the activation signal back to the layer behind\n",
    "    input_gradient = segment_inputs[-1].grad.clone()\n",
    "    if input_gradient.location != models[len(models)-2].location:\n",
    "        input_gradient.move(models[len(models)-2].location)\n",
    "        \n",
    "#     if segment_outputs[-2].location != models[len(models)-2].location:\n",
    "#         segment_outputs[-2].move(models[len(models)-2].location)\n",
    "    \n",
    "    # End layer updates weights\n",
    "    optimizers[-1].step()\n",
    "\n",
    "    # Compute Intermediary Layers: repeat the same operations\n",
    "    for iter in range(len(models)-1, 1, -1): \n",
    "        print(\"INTO FOR\")\n",
    "        optimizers[iter-1].zero_grad()\n",
    "                \n",
    "        intermediate_loss = torch.matmul(torch.t(segment_outputs[iter-1]), input_gradient).sum()\n",
    "        intermediate_loss.backward()\n",
    "        \n",
    "        print(intermediate_loss.location)\n",
    "        print(input_gradient.location)\n",
    "        print(segment_inputs[iter-1].grad)\n",
    "        print(segment_inputs[iter-1].location)\n",
    "\n",
    "\n",
    "        input_gradient = segment_inputs[iter-1].grad.clone()\n",
    "        if input_gradient.location != models[iter-2].location:\n",
    "            input_gradient.move(models[iter-2].location)\n",
    "            \n",
    "#         if segment_outputs[iter-2].location != models[iter-2].location:\n",
    "#             segment_outputs[iter-2].move(models[iter-2].location)\n",
    "\n",
    "        \n",
    "#         input_gradient = segment_inputs[iter-1].grad.clone().get().send(models[iter-2].location)\n",
    "        optimizers[iter-1].step()\n",
    "        print(iter)\n",
    "\n",
    "        \n",
    "\n",
    "    # Compute Final Layer, same but now input is the real input data\n",
    "    optimizers[0].zero_grad()\n",
    "    segment_output = segment_outputs[0]\n",
    "    intermediate_loss = torch.matmul(torch.t(segment_output), input_gradient).sum()\n",
    "    intermediate_loss.backward()\n",
    "    optimizers[0].step()\n",
    "        \n",
    "    return segment_outputs[-1], loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.5 - Run Training Logic\n",
    "\n",
    "Now we will run the training process over 200 epochs for each data owner. Every 20 epochs we will print our progress. The front and end sections of the model will be swapped between data owners training each individual batch.\n",
    "\n",
    "<img src=\"images/BatchFlow.png\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitNN_train(models, xyChain):\n",
    "    \n",
    "    #   Variables for performance metrics\n",
    "    start_time = time.time()\n",
    "    epochs = 300\n",
    "    lr = 0.2\n",
    "    counter = 0\n",
    "    \n",
    "    # Create optimisers for each segment and link to their segment\n",
    "    optimizers = [\n",
    "        optim.SGD(params=model.parameters(),lr=lr)\n",
    "        for model in models\n",
    "    ]\n",
    "    \n",
    "    for i, local_worker in enumerate(workers):\n",
    "        \n",
    "        # Begin work on current data subject\n",
    "        dataset = datasets[i]\n",
    "        \n",
    "        print('*', dataset.location.id, models[0].location.id)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward propogate through network until final layer is reached\n",
    "            segment_inputs, segment_outputs = forward(models, dataset.data)\n",
    "            \n",
    "            # Backward propogate\n",
    "            predictions, loss = backward(models, optimizers, segment_inputs, segment_outputs, dataset)\n",
    "\n",
    "            if epoch % 30 == 0:\n",
    "                print(f\"Epoch: {epoch}/{epochs} \\tLoss: \", \"{:.4f}\\tRuntime: {:.2f}s\".format(loss.get().data, time.time() - start_time))\n",
    "        \n",
    "        # If we are not at the end of the data owner chain send perimeter segments to next data owner\n",
    "        if i < len(workers)-1:\n",
    "            models[0].get().send(datasets[i+1].location)\n",
    "            models[len(models)-1].get().send(datasets[i+1].location)      \n",
    "            \n",
    "\n",
    "            print(\"\\nNEXT DATA OWNER\\n\")\n",
    "            print(\"MODEL CHAIN LOCATIONS\")\n",
    "            for iter in range(len(models)):\n",
    "                print(models[iter].location.id)  \n",
    "            print(\"\\n\")\n",
    "    \n",
    "    # Send models back to researcher\n",
    "    [model.get() for model in models]\n",
    "    \n",
    "    # Perform predictions with updates weights\n",
    "    out = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "    for i in range(len(models)):\n",
    "        out = models[i](out)\n",
    "        \n",
    "    print(\"\\n\\nFinal Predictions:\", torch.t(out).data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* alice alice\n",
      "INTO FOR\n",
      "<VirtualWorker id:claire #objects:9>\n",
      "<VirtualWorker id:claire #objects:9>\n",
      "(Wrapper)>[PointerTensor | me:77607064668 -> claire:70104102600]::grad\n",
      "<VirtualWorker id:claire #objects:9>\n",
      "4\n",
      "INTO FOR\n",
      "<VirtualWorker id:bob #objects:9>\n",
      "<VirtualWorker id:bob #objects:9>\n",
      "(Wrapper)>[PointerTensor | me:67038186143 -> bob:32974751714]::grad\n",
      "<VirtualWorker id:bob #objects:9>\n",
      "3\n",
      "INTO FOR\n",
      "<VirtualWorker id:alice #objects:20>\n",
      "<VirtualWorker id:alice #objects:20>\n",
      "None\n",
      "<VirtualWorker id:alice #objects:20>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'clone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5a97f10b075b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplitNN_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0a952d043bf4>\u001b[0m in \u001b[0;36msplitNN_train\u001b[0;34m(models, xyChain)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Backward propogate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-57c424a4b47c>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(models, optimizers, segment_inputs, segment_outputs, dataset)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0minput_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0minput_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clone'"
     ]
    }
   ],
   "source": [
    "splitNN_train(models, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- I figured out why move() was breaking the forward prop function; it was trying to send to itself which caused an error.\n",
    "- Gradient system breaks when not moving input tensors.\n",
    "- Problems of 'knowledge of other models' in the training process can be solved by reversing the order from Claire to Alice. This removes the need for encrypting model segments.\n",
    "- After I get to the bottom of the gradients, I will plug this into the MNIST and CIFAR datasets and for benchmarking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
