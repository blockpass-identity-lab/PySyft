{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.1 - A Distributed Training Example\n",
    "\n",
    "We will train a splitNN model that has been distributed to three different hosts. One host, Alice, is the data subject. Alice has the labelled data and will also be the custodian of the network start and end segments. Claire and Bob are worker hosts. They will feed the activation signals from the start of the chain forward until it reaches alices end layer. They will do the reverse with gradients in the backpropogation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.1 - Set up environmental variables\n",
    "\n",
    "Here we will import our required libraries and initialise our model segments and data. We will need;\n",
    "\n",
    "<img src=\"images/distributed.png\" width=\"50%\">\n",
    "\n",
    "- A dummy distributed dataset\n",
    "- 5 model segments\n",
    "- 3 Virtual Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import syft as sy\n",
    "import time\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "#from torchviz import make_dot, make_dot_from_trace\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def location(self):\n",
    "    m = self.__getitem__(0)\n",
    "    w = m.weight[0]\n",
    "    return w.location\n",
    "\n",
    "nn.Sequential.location = location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "x = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "y = torch.tensor([[0],[0],[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[1],[1.]])\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define 5 chained models\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(4, 3),\n",
    "        nn.Tanh()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(3, 3),\n",
    "        nn.Sigmoid()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(3, 3),\n",
    "        nn.Sigmoid()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(3, 2),\n",
    "        nn.Tanh()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(2, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "]\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "claire = sy.VirtualWorker(hook, id=\"claire\")\n",
    "workers = alice, bob, claire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final predictions are shown above, we can compare this with the output of the same 'split' neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.2 - Send Variables to Starting Locations\n",
    "\n",
    "In this example, Alice is the worker with the data and labels. Bob and Claire are intermediary hosts in the chain. Alice has the start and end model segments. Bob and Claire have intermediary segments.\n",
    "\n",
    "We send the models and data to their respective hosts and store the pointers in associative arrays; the Model Chain (MC) and the xy Chain (xyC). These contain the locations of the data, but no actual values. These are the only necessary parameters for coordinating this learning process. A summary of this is seen below\n",
    "\n",
    "<img src=\"images/Parameters.png\" width=\"50%\">\n",
    "\n",
    "In this experiment, the models and data are initialised locally and then distributed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Send Model Segments to starting locations\n",
    "model_locations = [alice, alice, bob, claire, alice]\n",
    "\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)\n",
    "\n",
    "# Create a remote copy of the dataset for each worker\n",
    "datasets = [\n",
    "    sy.BaseDataset(x.send(worker), y.send(worker))\n",
    "    for worker in (alice, bob, claire)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.3 - Forward Propogation\n",
    "\n",
    "We will need to define the logic of forward and backward propogation. \n",
    "\n",
    "Forward propogation feeds the input data into Alice's segment at the beginning of the chain. Alice then sends her activation signal to the location of the next model in the chain. This model propogates this activation and sends it onward to the location of the next segment. The signal will eventually reach alice's end segment to perform a prediction. We store pointers to the activations of each layer using the Activation Chain (AC). This allows us to retrieve the values when processing gradients. When the activations have fully propogated the MC, the method returns the resultant AC for use in the backpropogation function. \n",
    "\n",
    "<img src=\"images/activationchain.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.4 - Backward Propogation\n",
    "\n",
    "The backpropogation function takes the MC, xyC and AC as input parameters.\n",
    "\n",
    "<img src=\"images/backpropParams.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "First the backpropogation algorithm computes the loss on Alice's prediction. We use <b>**** what seems to be**** </b> the sum of squared error as our loss function.\n",
    "\n",
    "<img src=\"images/loss.png\" width=\"100%\">\n",
    "\n",
    "We then calculate the gradients for the parameters of the end segment using the chain rule.\n",
    "\n",
    "<img src=\"images/chainRule.png\" width=\"40%\">\n",
    "\n",
    "This is done automatically for the layers in each segment but we have to recalculate loss for each model segment during the backpropogation phase.\n",
    "\n",
    "<img src=\"images/intermediateLoss.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Each segment feeds the gradients of their activation function back to the segment behind them and updates their weights w.r.t these gradients. This layer computes it's loss by dot joining the orignal activation signal and it's new gradient. The sum of the result is used to feed back error down the line. After each segment is complete, the optimiser for that model updates. The process is repeated until the segment at the beginning of the chain is reached and alice updates the gradients on her beginning segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(models, x):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    # First: provide x as input\n",
    "    inputs.append(x)\n",
    "    outputs.append(models[0](x))    \n",
    "    \n",
    "#     Update: Move() can crash if self-referencing,\n",
    "#             This has been fixed with an if-statement.\n",
    "#             If a variable is not to be moved, it \n",
    "#             isn't moved before filling the next input.\n",
    "            \n",
    "#             However, not moving the output seems to break\n",
    "#             the grad system in the backprop function; leaving\n",
    "#             no gradients attached to this after backward()\n",
    "#             function. \n",
    "            \n",
    "#             Oddly this doesn't happen when the tensor is moved.\n",
    "            \n",
    "    if outputs[-1].location != models[1].location:\n",
    "        next_input = outputs[-1].copy().move(models[1].location)\n",
    "    else:\n",
    "        next_input = outputs[-1].copy()\n",
    "#         next_input = outputs[-1].copy().get().send(models[1].location)\n",
    "        \n",
    "    for i in range(1, len(models)-1):\n",
    "        inputs.append(next_input)\n",
    "        outputs.append(models[i](next_input))\n",
    "        next_input = outputs[-1].copy().get().send(models[i+1].location)\n",
    " \n",
    "    # Last: don't move the result to the next location\n",
    "    inputs.append(next_input)\n",
    "    outputs.append(models[len(models)-1](next_input))\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(models, optimizers, segment_inputs, segment_outputs, dataset):\n",
    "    data, targets = dataset.data, dataset.targets\n",
    "        \n",
    "    # Destroy pre-existing gradient of final layer\n",
    "    optimizers[len(optimizers)-1].zero_grad()\n",
    "   \n",
    "    loss = (((segment_outputs[-1] - targets)**2).sum())\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # End layer sends the gradient of the activation signal back to the layer behind\n",
    "    input_gradient = segment_inputs[-1].grad.clone().get().send(models[len(models)-2].location)\n",
    "    \n",
    "    # End layer updates weights\n",
    "    optimizers[-1].step()\n",
    "\n",
    "    # Compute Intermediary Layers: repeat the same operations\n",
    "    for iter in range(len(models)-1, 1, -1): \n",
    "        optimizers[iter-1].zero_grad()\n",
    "        \n",
    "        intermediate_loss = torch.matmul(torch.t(segment_outputs[iter-1]), input_gradient).sum()\n",
    "        intermediate_loss.backward()\n",
    "        \n",
    "        print(iter)\n",
    "        if iter == 2 and segment_inputs[iter-1].grad == None:\n",
    "            print(\"BREAKS ON THIS SEGMENT: Processing input which wasn't moved. \\n Try uncommenting the old .get().send() command in the forward propogation function\")\n",
    "            \n",
    "        input_gradient = segment_inputs[iter-1].grad.clone().get().send(models[iter-2].location)\n",
    "        optimizers[iter-1].step()\n",
    "\n",
    "    # Compute Final Layer, same but now input is the real input data\n",
    "    optimizers[0].zero_grad()\n",
    "    segment_output = segment_outputs[0]\n",
    "    intermediate_loss = torch.matmul(torch.t(segment_output), input_gradient).sum()\n",
    "    intermediate_loss.backward()\n",
    "    optimizers[0].step()\n",
    "        \n",
    "    return segment_outputs[-1], loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1.5 - Run Training Logic\n",
    "\n",
    "Now we will run the training process over 200 epochs for each data owner. Every 20 epochs we will print our progress. The front and end sections of the model will be swapped between data owners training each individual batch.\n",
    "\n",
    "<img src=\"images/BatchFlow.png\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitNN_train(models, xyChain):\n",
    "    \n",
    "    #   Variables for performance metrics\n",
    "    start_time = time.time()\n",
    "    epochs = 300\n",
    "    lr = 0.2\n",
    "    counter = 0\n",
    "    \n",
    "    # Create optimisers for each segment and link to their segment\n",
    "    optimizers = [\n",
    "        optim.SGD(params=model.parameters(),lr=lr)\n",
    "        for model in models\n",
    "    ]\n",
    "    \n",
    "    for i, local_worker in enumerate(workers):\n",
    "        \n",
    "        # Begin work on current data subject\n",
    "        dataset = datasets[i]\n",
    "        \n",
    "        print('*', dataset.location.id, models[0].location.id)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward propogate through network until final layer is reached\n",
    "            segment_inputs, segment_outputs = forward(models, dataset.data)\n",
    "            \n",
    "            # Backward propogate\n",
    "            predictions, loss = backward(models, optimizers, segment_inputs, segment_outputs, dataset)\n",
    "\n",
    "            if epoch % 30 == 0:\n",
    "                print(f\"Epoch: {epoch}/{epochs} \\tLoss: \", \"{:.4f}\\tRuntime: {:.2f}s\".format(loss.get().data, time.time() - start_time))\n",
    "        \n",
    "        # If we are not at the end of the data owner chain send perimeter segments to next data owner\n",
    "        if i < len(workers)-1:\n",
    "            models[0].get().send(datasets[i+1].location)\n",
    "            models[len(models)-1].get().send(datasets[i+1].location)      \n",
    "            \n",
    "\n",
    "            print(\"\\nNEXT DATA OWNER\\n\")\n",
    "            print(\"MODEL CHAIN LOCATIONS\")\n",
    "            for iter in range(len(models)):\n",
    "                print(models[iter].location.id)  \n",
    "            print(\"\\n\")\n",
    "    \n",
    "    # Send models back to researcher\n",
    "    [model.get() for model in models]\n",
    "    \n",
    "    # Perform predictions with updates weights\n",
    "    out = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "    for i in range(len(models)):\n",
    "        out = models[i](out)\n",
    "        \n",
    "    print(\"\\n\\nFinal Predictions:\", torch.t(out).data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* alice alice\n",
      "4\n",
      "3\n",
      "2\n",
      "BREAKS ON THIS SEGMENT: Processing input which wasn't moved. \n",
      " Try uncommenting the old .get().send() command in the forward propogation function\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'clone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5a97f10b075b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplitNN_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-0a952d043bf4>\u001b[0m in \u001b[0;36msplitNN_train\u001b[0;34m(models, xyChain)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Backward propogate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7ce09ac69ff8>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(models, optimizers, segment_inputs, segment_outputs, dataset)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BREAKS ON THIS SEGMENT: Processing input which wasn't moved. \\n Try uncommenting the old .get().send() command in the forward propogation function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0minput_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clone'"
     ]
    }
   ],
   "source": [
    "splitNN_train(models, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- I figured out why move() was breaking the forward prop function; it was trying to send to itself which caused an error.\n",
    "- Gradient system breaks when not moving input tensors.\n",
    "- Problems of 'knowledge of other models' in the training process can be solved by reversing the order from Claire to Alice. This removes the need for encrypting model segments.\n",
    "- After I get to the bottom of the gradients, I will plug this into the MNIST and CIFAR datasets and for benchmarking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
