{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5702)\n",
      "tensor(3.4189)\n",
      "tensor(3.2462)\n",
      "tensor(3.0390)\n",
      "tensor(2.7925)\n",
      "tensor(2.5109)\n",
      "tensor(2.2076)\n",
      "tensor(1.9019)\n",
      "tensor(1.6136)\n",
      "tensor(1.3571)\n",
      "tensor(1.1390)\n",
      "tensor(0.9591)\n",
      "tensor(0.8130)\n",
      "tensor(0.6952)\n",
      "tensor(0.6001)\n",
      "tensor(0.5230)\n",
      "tensor(0.4599)\n",
      "tensor(0.4080)\n",
      "tensor(0.3648)\n",
      "tensor(0.3285)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# A Toy Dataset\n",
    "data = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "target = torch.tensor([[0],[0],[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[1],[1.]])\n",
    "\n",
    "\n",
    "# A Toy Model\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(4, 3),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(3, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "def train():\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for iter in range(20):\n",
    "\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        pred = model(data)\n",
    "\n",
    "        # 3) calculate how much we missed\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) change those weights\n",
    "        opt.step()\n",
    "\n",
    "        # 6) print our progress\n",
    "        print(loss.data)\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adamhall/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import syft as sy\n",
    "import time\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# from torchviz import make_dot, make_dot_from_trace\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# A Toy Dataset\n",
    "x = torch.tensor([[0,0,0,0],[1,0,0,0],[0,1,0,0],[0,0,1,0],[1,1,0,0],[1,0,1,0],[0,1,1,0],[1,1,1,0],[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,1],[0,1,1,1],[1,1,1,1.]])\n",
    "y = torch.tensor([[0],[0],[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[1],[1.]])\n",
    "\n",
    "#   Variables for performance metrics\n",
    "start_time = time.time()\n",
    "epochs = 300\n",
    "lr = 0.2\n",
    "counter = 0\n",
    "\n",
    "\n",
    "# Define 2 chained models\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(4, 3),\n",
    "        nn.Tanh()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(3, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create optimisers for each segment and link to their segment\n",
    "optimizers = [\n",
    "    optim.SGD(params=model.parameters(),lr=lr)\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "workers = alice, bob\n",
    "\n",
    "# Send Model Segments to starting locations\n",
    "model_locations = [alice, bob]\n",
    "\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)\n",
    "\n",
    "# Create a remote copy of the dataset for each worker\n",
    "datasets = [\n",
    "    sy.BaseDataset(x.send(alice), y.send(bob))\n",
    "    for worker in workers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(models, x):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    # First: provide x as input\n",
    "    inputs.append(x)\n",
    "    outputs.append(models[0](inputs[-1]))\n",
    "    \n",
    "    # MOve a copy of the inputs from the previous layer to the layer in front\n",
    "    inputs.append(outputs[-1].copy().move(bob))\n",
    "    outputs.append(models[1](inputs[-1]))\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(models, optimizers, inputs, outputs, dataset):\n",
    "    data, targets = dataset.data, dataset.targets\n",
    "        \n",
    "    # Destroy pre-existing gradient of final layer\n",
    "    optimizers[-1].zero_grad()\n",
    "    loss = (((outputs[-1] - targets)**2).sum())\n",
    "    loss.backward()\n",
    "    # End layer sends the gradient of the activation signal back to the layer behind\n",
    "    input_gradient = inputs[-1].grad.clone().move(alice)\n",
    "    # End layer updates weights\n",
    "    optimizers[1].step()\n",
    "\n",
    "    # Compute Final Layer, same but now input is the real input data\n",
    "    optimizers[0].zero_grad()\n",
    "    segment_output = outputs[0]\n",
    "    # Dot join the gradient of the input to the layer in front to the output of this segment\n",
    "    intermediate_loss = torch.matmul(torch.t(segment_output), input_gradient).sum()\n",
    "    intermediate_loss.backward()\n",
    "    optimizers[0].step()\n",
    "        \n",
    "    return segment_output[-1], loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3027, requires_grad=True)\n",
      "tensor(4.0265, requires_grad=True)\n",
      "tensor(3.8157, requires_grad=True)\n",
      "tensor(3.6322, requires_grad=True)\n",
      "tensor(3.4653, requires_grad=True)\n",
      "tensor(3.3123, requires_grad=True)\n",
      "tensor(3.1722, requires_grad=True)\n",
      "tensor(3.0442, requires_grad=True)\n",
      "tensor(2.9272, requires_grad=True)\n",
      "tensor(2.8203, requires_grad=True)\n",
      "tensor(2.7225, requires_grad=True)\n",
      "tensor(2.6330, requires_grad=True)\n",
      "tensor(2.5509, requires_grad=True)\n",
      "tensor(2.4754, requires_grad=True)\n",
      "tensor(2.4058, requires_grad=True)\n",
      "tensor(2.3416, requires_grad=True)\n",
      "tensor(2.2822, requires_grad=True)\n",
      "tensor(2.2270, requires_grad=True)\n",
      "tensor(2.1757, requires_grad=True)\n",
      "tensor(2.1279, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    inputs, outputs = forward(models, datasets[0].data)\n",
    "    prediction, loss = backward(models, optimizers, inputs, outputs, datasets[0])\n",
    "    print(loss.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am looking train this model while it is split across two hosts. I attempt to transfer loss backward for the backpropogation by sending the gradients to join to the output of the previous layer. This works but not as well as the other model which is centralised. I am wondering how i can improve this loss transfer formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
